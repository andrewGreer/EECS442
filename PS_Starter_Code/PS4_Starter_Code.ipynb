{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1ixH5Y94h48t3fzDmrWEf0Khk1ssCfM-V","timestamp":1703323620214}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"ix5dQS2rUMlu"},"source":["#EECS 442 PS4: Backpropagation\n","\n","**Please make a copy of this file before editing!**\n","`File > Save a copy in Drive`\n","\n","__Please provide the following information__\n","(e.g. Andrew Owens, ahowens):\n","\n","[Your first name] [Your last name], [Your UMich uniqname]\n","\n","__Important__: after you download the .ipynb file, please name it __\\<your_uniqname\\>_\\<your_umid\\>.ipynb__ before you submit it to Canvas.\n","\n","Example: ahowens_00000000.ipynb.\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"W_Cst4k4tuBc"},"source":["# Starting\n","\n","Run the following code to import the modules you'll need. After your finish the assignment, remember to run all cells and save the note book to your local machine as a .ipynb file for Canvas submission."]},{"cell_type":"code","metadata":{"id":"SHumIO-xt57H"},"source":["import pickle\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import os\n","import math\n","from torchvision.datasets import CIFAR10\n","download = not os.path.isdir('cifar-10-batches-py')\n","dset_train = CIFAR10(root='.', download=download)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"98aoizX_UG-W"},"source":["# Problem 4.1 Understanding Backpropagation\n"]},{"cell_type":"markdown","metadata":{"id":"1IyQj_N-UPC-"},"source":["# 4.1 (b)  \n","Implement the code for forward and backward pass of computation graph in (a)"]},{"cell_type":"code","metadata":{"id":"GIOVvqcPUf1-"},"source":["def f_1(x0, x1, x2, w0, w1, w2, w3):\n","    \"\"\"\n","    Computes the forward and backward pass through the computational graph\n","    of (a)\n","\n","    Inputs:\n","    - x0, x1, w0, w1, w2: Python floats\n","\n","    Returns a tuple of:\n","    - L: The output of the graph\n","    - grads: A tuple (grad_x0, grad_x1, grad_x2, grad_w0, grad_w1, grad_w2, grad_w2)\n","    giving the derivative of the output L with respect to each input.\n","    \"\"\"\n","    ###########################################################################\n","    # TODO: Implement the forward pass for the computational graph for (a) and#\n","    # store the output of this graph as L                                     #\n","    ###########################################################################\n","\n","    ###########################################################################\n","    #                              END OF YOUR CODE                           #\n","    ###########################################################################\n","\n","\n","\n","    ###########################################################################\n","    # TODO: Implement the backward pass for the computational graph for (a)   #\n","    # Store the gradients for each input                                      #\n","    ###########################################################################\n","\n","    ###########################################################################\n","    #                              END OF YOUR CODE                           #\n","    ###########################################################################\n","\n","    grads = (grad_x0, grad_x1, grad_x2, grad_w0, grad_w1, grad_w2, grad_w3)\n","    return L, grads"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ErRvlYzAYrTm"},"source":["# 4.1 (c)  \n","Implement the code for forward and backward pass of computation graph in (c)\n"]},{"cell_type":"code","metadata":{"id":"tmh5UlxIY0re"},"source":["def f_2(x, y, z, w):\n","    \"\"\"\n","    Computes the forward and backward pass through the computational graph\n","    of (c)\n","\n","    Inputs:\n","    - x, y, z: Python floats\n","\n","    Returns a tuple of:\n","    - L: The output of the graph\n","    - grads: A tuple (grad_x, grad_y, grad_z, grad_w)\n","    giving the derivative of the output L with respect to each input.\n","    \"\"\"\n","    ###########################################################################\n","    # TODO: Implement the forward pass for the computational graph for (c) and#\n","    # store the output of this graph as L                                     #\n","    ###########################################################################\n","\n","    ###########################################################################\n","    #                              END OF YOUR CODE                           #\n","    ###########################################################################\n","\n","\n","\n","    ###########################################################################\n","    # TODO: Implement the backward pass for the computational graph for (c)   #\n","    # Store the gradients for each input                                      #\n","    ###########################################################################\n","\n","    ###########################################################################\n","    #                              END OF YOUR CODE                           #\n","    ###########################################################################\n","\n","    grads = (grad_x, grad_y, grad_z, grad_w)\n","    return L, grads"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"apEPzDNtK0MC"},"source":["# Problem 4.2 Softmax Classifier with Two Layer Neural Network\n","In this problem you will develop a two Layer neural network with fully-connected layers to perform classification, and test it out on the CIFAR-10 dataset.\n","\n","We train the network with a softmax loss function on the weight matrices. The network uses a ReLU nonlinearity after the first fully connected layer. In other words, the network has the following architecture:\n","\n","input - fully connected layer - ReLU - fully connected layer - softmax\n","\n","The outputs of the second fully-connected layer are the scores for each class.\n","\n","You cannot use any deep learning libraries such as PyTorch in this part."]},{"cell_type":"markdown","metadata":{"id":"SXfumCQ21JoK"},"source":["# 4.2 (a) Layers\n","In this problem, implement fully connected layer, relu. Softmax layer has already been implemented in the provided code. Filling in all TODOs in skeleton codes will be sufficient."]},{"cell_type":"code","metadata":{"id":"Q-ljfgMv9PHx"},"source":["def fc_forward(X, W, b):\n","    \"\"\"\n","    Computes the forward pass for a fully-connected layer.\n","\n","    The input X has shape (N, Din) and contains a minibatch of N\n","    examples, where each example x[i] has shape (Din,).\n","\n","    Inputs:\n","    - X: A numpy array containing input data, of shape (N, Din)\n","    - W: A numpy array of weights, of shape (Din, Dout)\n","    - b: A numpy array of biases, of shape (Dout,)\n","\n","    Returns a tuple of:\n","    - out: output, of shape (N, Dout)\n","    - cache: (X, W, b)\n","    \"\"\"\n","    ###########################################################################\n","    # TODO: Implement the forward pass. Store the result in out.              #\n","    ###########################################################################\n","\n","    ###########################################################################\n","    #                             END OF YOUR CODE                            #\n","    ###########################################################################\n","    cache = (X, W, b)\n","    return out, cache\n","\n","\n","def fc_backward(dout, cache):\n","    \"\"\"\n","    Computes the backward pass for a fully_connected layer.\n","\n","    Inputs:\n","    - dout: Upstream derivative, of shape (N, Dout)\n","    - cache: returned by your forward function. Tuple of:\n","      - X: Input data, of shape (N, Din)\n","      - W: Weights, of shape (Din, Dout)\n","      - b: Biases, of shape (Dout,)\n","\n","    Returns a tuple of:\n","    - dX: Gradient with respect to X, of shape (N, Din)\n","    - dW: Gradient with respect to W, of shape (Din, Dout)\n","    - db: Gradient with respect to b, of shape (Dout,)\n","    \"\"\"\n","    X, W, b = cache\n","    dX, dW, db = None, None, None\n","    ###########################################################################\n","    # TODO: Implement the affine backward pass.                               #\n","    ###########################################################################\n","\n","    ###########################################################################\n","    #                             END OF YOUR CODE                            #\n","    ###########################################################################\n","    return dX, dW, db\n","\n","def relu_forward(x):\n","    \"\"\"\n","    Computes the forward pass for a layer of rectified linear units (ReLUs).\n","\n","    Input:\n","    - x: Inputs, of any shape\n","\n","    Returns a tuple of:\n","    - out: Output, of the same shape as x\n","    - cache: x\n","    \"\"\"\n","    out = x.copy()\n","    ###########################################################################\n","    # TODO: Implement the ReLU forward pass.                                  #\n","    ###########################################################################\n","\n","    ###########################################################################\n","    #                             END OF YOUR CODE                            #\n","    ###########################################################################\n","    cache = x\n","    return out, cache\n","\n","\n","def relu_backward(dout, cache):\n","    \"\"\"\n","    Computes the backward pass for a layer of rectified linear units (ReLUs).\n","\n","    Input:\n","    - dout: Upstream derivatives, of any shape\n","    - cache: returned by your forward function. Input x, of same shape as dout\n","\n","    Returns:\n","    - dx: Gradient with respect to x\n","    \"\"\"\n","    dx, x = dout.copy(), cache\n","    ###########################################################################\n","    # TODO: Implement the ReLU backward pass.                                 #\n","    ###########################################################################\n","\n","    ###########################################################################\n","    #                             END OF YOUR CODE                            #\n","    ###########################################################################\n","    return dx\n","\n","\n","def softmax_loss(X, y):\n","    \"\"\"\n","    Computes the loss and gradient for softmax classification.\n","\n","    Inputs:\n","    - X: Input data, of shape (N, C) where x[i, j] is the score for the jth\n","      class for the ith input.\n","    - y: Vector of labels, of shape (N,) where y[i] is the label for X[i] and\n","      0 <= y[i] < C\n","\n","    Returns a tuple of:\n","    - loss: Scalar giving the loss\n","    - dX: Gradient of the loss with respect to x\n","    \"\"\"\n","    loss, dX = None, None                                         #\n","    dX = np.exp(X - np.max(X, axis=1, keepdims=True))\n","    dX /= np.sum(dX, axis=1, keepdims=True)\n","    loss = -np.sum(np.log(dX[np.arange(X.shape[0]), y])) / X.shape[0]\n","    dX[np.arange(X.shape[0]), y] -= 1\n","    dX /= X.shape[0]\n","\n","    return loss, dX"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LbFxtS3zK8oz"},"source":["# 4.2 (b) Softmax Classifier\n","\n","In this problem, implement softmax classifier."]},{"cell_type":"code","metadata":{"id":"ytvxbx9UpxVL"},"source":["class SoftmaxClassifier(object):\n","    \"\"\"\n","    A fully-connected neural network with\n","    softmax loss that uses a modular layer design. We assume an input dimension\n","    of D, a hidden dimension of H, and perform classification over C classes.\n","\n","    The architecture should be fc - relu - fc - softmax with one hidden layer\n","\n","    The learnable parameters of the model are stored in the dictionary\n","    self.params that maps parameter names to numpy arrays.\n","    \"\"\"\n","\n","    def __init__(self, input_dim=3072, hidden_dim=300, num_classes=10,\n","                 weight_scale=1e-3):\n","        \"\"\"\n","        Initialize a new network.\n","\n","        Inputs:\n","        - input_dim: An integer giving the size of the input\n","        - hidden_dim: An integer giving the size of the hidden layer, None\n","          if there's no hidden layer.\n","        - num_classes: An integer giving the number of classes to classify\n","        - weight_scale: Scalar giving the standard deviation for random\n","          initialization of the weights.\n","        \"\"\"\n","        self.params = {}\n","        ############################################################################\n","        # TODO: Initialize the weights and biases of the two-layer net. Weights    #\n","        # should be initialized from a Gaussian centered at 0.0 with               #\n","        # standard deviation equal to weight_scale, and biases should be           #\n","        # initialized to zero. All weights and biases should be stored in the      #\n","        # dictionary self.params, with fc weights and biases using the keys        #\n","        # 'W' and 'b', i.e., W1, b1 for the weights and bias in the first linear   #\n","        # layer, W2, b2 for the weights and bias in the second linear layer.       #\n","        ############################################################################\n","\n","        ############################################################################\n","        #                             END OF YOUR CODE                             #\n","        ############################################################################\n","\n","\n","    def forwards_backwards(self, X, y=None):\n","        \"\"\"\n","        Compute loss and gradient for a minibatch of data.\n","\n","        Inputs:\n","        - X: Array of input data of shape (N, Din)\n","        - y: Array of labels, of shape (N,). y[i] gives the label for X[i].\n","\n","        Returns:\n","        If y is None, then run a test-time forward pass of the model and return:\n","        - scores: Array of shape (N, C) giving classification scores, where\n","          scores[i, c] is the classification score for X[i] and class c.\n","\n","        If y is not None, then run a training-time forward and backward pass. And\n","        return a tuple of:\n","        - loss: Scalar value giving the loss\n","        - grads: Dictionary with the same keys as self.params, mapping parameter\n","          names to gradients of the loss with respect to those parameters.\n","        \"\"\"\n","        scores = None\n","        ############################################################################\n","        # TODO: Implement the forward pass for the two-layer net, computing the    #\n","        # class scores for X and storing them in the scores variable.              #\n","        ############################################################################\n","\n","        ############################################################################\n","        #                             END OF YOUR CODE                             #\n","        ############################################################################\n","\n","        # If y is None then we are in test mode so just return scores\n","        if y is None:\n","            return scores\n","\n","        loss, grads = 0, {}\n","        ############################################################################\n","        # TODO: Implement the backward pass for the two-layer net. Store the loss  #\n","        # in the loss variable and gradients in the grads dictionary. Compute data #\n","        # loss using softmax, and make sure that grads[k] holds the gradients for  #\n","        # self.params[k].                                                          #\n","        ############################################################################\n","\n","        ############################################################################\n","        #                             END OF YOUR CODE                             #\n","        ############################################################################\n","        return loss, grads\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lwp0waIL1h_e"},"source":["# 4.2(c) Training\n","\n","In this problem, you need to preprocess the images and set up model hyperparameters. Notice that adjust the training and val split is optional."]},{"cell_type":"code","metadata":{"id":"kZPtQzXGMoCg"},"source":["def unpickle(file):\n","    with open(file, 'rb') as fo:\n","        dict = pickle.load(fo, encoding=\"latin1\")\n","    return dict\n","\n","def load_cifar10():\n","    data = {}\n","    meta = unpickle(\"cifar-10-batches-py/batches.meta\")\n","    batch1 = unpickle(\"cifar-10-batches-py/data_batch_1\")\n","    batch2 = unpickle(\"cifar-10-batches-py/data_batch_2\")\n","    batch3 = unpickle(\"cifar-10-batches-py/data_batch_3\")\n","    batch4 = unpickle(\"cifar-10-batches-py/data_batch_4\")\n","    batch5 = unpickle(\"cifar-10-batches-py/data_batch_5\")\n","    test_batch = unpickle(\"cifar-10-batches-py/test_batch\")\n","    X_train = np.vstack((batch1['data'], batch2['data'], batch3['data'],\\\n","                         batch4['data'], batch5['data']))\n","    Y_train = np.array(batch1['labels'] + batch2['labels'] + batch3['labels'] +\n","                       batch4['labels'] + batch5['labels'])\n","    X_test = test_batch['data']\n","    Y_test = test_batch['labels']\n","\n","    #Preprocess images here\n","    X_train = (X_train-np.mean(X_train,axis=1,keepdims=True))/np.std(X_train,axis=1,keepdims=True)\n","    X_test = (X_test-np.mean(X_test,axis=1,keepdims=True))/np.std(X_test,axis=1,keepdims=True)\n","\n","    data['X_train'] = X_train[:40000]\n","    data['y_train'] = Y_train[:40000]\n","    data['X_val'] = X_train[40000:]\n","    data['y_val'] = Y_train[40000:]\n","    data['X_test'] = X_test\n","    data['y_test'] = Y_test\n","    return data\n","\n","def testNetwork(model, X, y, num_samples=None, batch_size=100):\n","    \"\"\"\n","    Check accuracy of the model on the provided data.\n","\n","    Inputs:\n","    - model: Image classifier\n","    - X: Array of data, of shape (N, d_1, ..., d_k)\n","    - y: Array of labels, of shape (N,)\n","    - num_samples: If not None, subsample the data and only test the model\n","      on num_samples datapoints.\n","    - batch_size: Split X and y into batches of this size to avoid using\n","      too much memory.\n","\n","    Returns:\n","    - acc: Scalar giving the fraction of instances that were correctly\n","      classified by the model.\n","    \"\"\"\n","\n","    # Subsample the data\n","    N = X.shape[0]\n","    if num_samples is not None and N > num_samples:\n","        mask = np.random.choice(N, num_samples)\n","        N = num_samples\n","        X = X[mask]\n","        y = y[mask]\n","\n","    # Compute predictions in batches\n","    num_batches = N // batch_size\n","    if N % batch_size != 0:\n","        num_batches += 1\n","    y_pred = []\n","    for i in range(num_batches):\n","        start = i * batch_size\n","        end = (i + 1) * batch_size\n","        scores = model.forwards_backwards(X[start:end])\n","        y_pred.append(np.argmax(scores, axis=1))\n","    y_pred = np.hstack(y_pred)\n","    acc = np.mean(y_pred == y)\n","\n","    return acc\n","\n","def SGD(W,dW, learning_rate=1e-3):\n","    \"\"\" Apply a gradient descent step on weight W\n","    Inputs:\n","        W : Weight matrix\n","        dW : gradient of weight, same shape as W\n","        learning_rate : Learning rate. Defaults to 1e-3.\n","    Returns:\n","        new_W: Updated weight matrix\n","    \"\"\"\n","\n","    # Apply a gradient descent step on weight W using the gradient dW and the specified learning rate.\n","    new_W = W - learning_rate * dW\n","\n","    return new_W\n","\n","def trainNetwork(model, data, **kwargs):\n","    \"\"\"\n","     Required arguments:\n","    - model: Image classifier\n","    - data: A dictionary of training and validation data containing:\n","      'X_train': Array, shape (N_train, d_1, ..., d_k) of training images\n","      'X_val': Array, shape (N_val, d_1, ..., d_k) of validation images\n","      'y_train': Array, shape (N_train,) of labels for training images\n","      'y_val': Array, shape (N_val,) of labels for validation images\n","\n","    Optional arguments:\n","    - learning_rate: A scalar for initial learning rate.\n","    - lr_decay: A scalar for learning rate decay; after each epoch the\n","      learning rate is multiplied by this value.\n","    - batch_size: Size of minibatches used to compute loss and gradient\n","      during training.\n","    - num_epochs: The number of epochs to run for during training.\n","    - print_every: Integer; training losses will be printed every\n","      print_every iterations.\n","    - verbose: Boolean; if set to false then no output will be printed\n","      during training.\n","    - num_train_samples: Number of training samples used to check training\n","      accuracy; default is 1000; set to None to use entire training set.\n","    - num_val_samples: Number of validation samples to use to check val\n","      accuracy; default is None, which uses the entire validation set.\n","    - optimizer: Choice of using either 'SGD' or 'SGD_Momentum' for updating weights; default is SGD.\n","    \"\"\"\n","\n","\n","    learning_rate =  kwargs.pop('learning_rate', 1e-3)\n","    lr_decay = kwargs.pop('lr_decay', 1.0)\n","    batch_size = kwargs.pop('batch_size', 100)\n","    num_epochs = kwargs.pop('num_epochs', 10)\n","    num_train_samples = kwargs.pop('num_train_samples', 1000)\n","    num_val_samples = kwargs.pop('num_val_samples', None)\n","    print_every = kwargs.pop('print_every', 10)\n","    verbose = kwargs.pop('verbose', True)\n","    optimizer = kwargs.pop('optimizer', 'SGD')\n","\n","    epoch = 0\n","    best_val_acc = 0\n","    best_params = {}\n","    loss_history = []\n","    train_acc_history = []\n","    val_acc_history = []\n","\n","\n","    num_train = data['X_train'].shape[0]\n","    iterations_per_epoch = max(num_train // batch_size, 1)\n","    num_iterations = num_epochs * iterations_per_epoch\n","\n","    #Initialize velocity dictionary if optimizer is SGD_Momentum\n","    if optimizer == 'SGD_Momentum':\n","      velocity_dict = {p:np.zeros(w.shape) for p,w in model.params.items()}\n","\n","    for t in range(num_iterations):\n","        # Make a minibatch of training data\n","        batch_mask = np.random.choice(num_train, batch_size)\n","        X_batch = data['X_train'][batch_mask]\n","        y_batch = data['y_train'][batch_mask]\n","\n","        # Compute loss and gradient\n","        loss, grads = model.forwards_backwards(X_batch, y_batch)\n","        loss_history.append(loss)\n","\n","        # Perform a parameter update\n","        if optimizer == 'SGD':\n","          for p, w in model.params.items():\n","              model.params[p] = SGD(w,grads[p], learning_rate=learning_rate)\n","\n","        elif optimizer == 'SGD_Momentum':\n","          for p, w in model.params.items():\n","              model.params[p], velocity_dict[p] = SGD_Momentum(w, grads[p], velocity_dict[p], beta=0.5, learning_rate=learning_rate)\n","        else:\n","          raise NotImplementedError\n","        # Print training loss\n","        if verbose and t % print_every == 0:\n","            print('(Iteration %d / %d) loss: %f' % (\n","                   t + 1, num_iterations, loss_history[-1]))\n","\n","        # At the end of every epoch, increment the epoch counter and decay\n","        # the learning rate.\n","        epoch_end = (t + 1) % iterations_per_epoch == 0\n","        if epoch_end:\n","            epoch += 1\n","            learning_rate *= lr_decay\n","\n","        # Check train and val accuracy on the first iteration, the last\n","        # iteration, and at the end of each epoch.\n","        first_it = (t == 0)\n","        last_it = (t == num_iterations - 1)\n","        if first_it or last_it or epoch_end:\n","            train_acc = testNetwork(model, data['X_train'], data['y_train'],\n","                num_samples= num_train_samples)\n","            val_acc = testNetwork(model, data['X_val'], data['y_val'],\n","                num_samples=num_val_samples)\n","            train_acc_history.append(train_acc)\n","            val_acc_history.append(val_acc)\n","\n","            if verbose:\n","                print('(Epoch %d / %d) train acc: %f; val_acc: %f' % (\n","                       epoch, num_epochs, train_acc, val_acc))\n","\n","            # Keep track of the best model\n","            if val_acc > best_val_acc:\n","                best_val_acc = val_acc\n","                best_params = {}\n","                for k, v in model.params.items():\n","                    best_params[k] = v.copy()\n","\n","    model.params = best_params\n","\n","    return model, train_acc_history, val_acc_history\n","\n","\n","# load data\n","data = load_cifar10()\n","train_data = { k: data[k] for k in ['X_train', 'y_train',\n","                                    'X_val', 'y_val']}\n","#######################################################################\n","# TODO: Set up model hyperparameters for SGD                               #\n","#######################################################################\n","\n","# initialize model\n","model_SGD = SoftmaxClassifier(hidden_dim = 300, weight_scale=1e-2)\n","\n","# start training using SGD\n","model_SGD, train_acc_history_SGD, val_acc_history_SGD = trainNetwork(\n","    model_SGD, train_data, learning_rate = ,\n","    lr_decay=, num_epochs=10,\n","    batch_size=, print_every=1000, optimizer = 'SGD')\n","#######################################################################\n","#                         END OF YOUR CODE                            #\n","#######################################################################\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e2ilTVXIw_7q"},"source":["# 4.2(d) Training with SGD_Momentum\n","\n","The model above was trained using SGD. Now implement the SGD_Momentum function to train the model using SGD with momentum."]},{"cell_type":"code","metadata":{"id":"54jGVPZOXtV6"},"source":["def SGD_Momentum(W, dW, velocity, beta=0.5, learning_rate=1e-3):\n","    \"\"\" Apply a gradient descent with momentum update on weight W\n","    Inputs:\n","        W : Weight matrix\n","        dW : gradient of weight, same shape as W\n","        velocity : velocity matrix, same shape as W\n","        beta : scalar value in range [0,1] weighting the velocity matrix. Setting it to 0 should make SGD_Momentum same as SGD.\n","               Defaults to 0.5.\n","        learning_rate : Learning rate. Defaults to 1e-3.\n","    Returns:\n","        new_W: Updated weight matrix\n","        new_velocity: Updated velocity matrix\n","    \"\"\"\n","    # ===== your code here! =====\n","    # TODO:\n","    # Apply a gradient descent step on weight W using the gradient dW and the specified learning rate.\n","    # 1. Calculate the new velocity by using the velocity of last iteration (input velocity) and gradient\n","    # 2. Update the weights using the new_velocity\n","\n","    # ==== end of code ====\n","    return new_W, new_velocity\n","\n","#######################################################################\n","# TODO: Set up model hyperparameters for SGD_Momentum\n","# Your hyperparameters should be identical to what you used for SGD (without momentum)#\n","#######################################################################\n","\n","# initialize model\n","model_SGD_Momentum = SoftmaxClassifier(hidden_dim = 300, weight_scale=1e-2)\n","\n","# start training\n","#Using SGD_Momentum as optimizer for trainning for training\n","model_SGD_Momentum, train_acc_history_SGD_Momentum, val_acc_history_SGD_Momentum = trainNetwork(\n","    model_SGD_Momentum, train_data, learning_rate = ,\n","    lr_decay=, num_epochs=10,\n","    batch_size=, print_every=1000, optimizer = 'SGD_Momentum')\n","#######################################################################\n","#                         END OF YOUR CODE                            #\n","#######################################################################"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fcovGmpXvXXa"},"source":["# 4.2(e) Report Accuracy\n","\n","Run the given code and report the accuracy of model_SGD and model_SGD_Momentum on test set. Which model trains more quickly? Is the ultimate validation accuracy different? Report your observation in the text block below.\n"]},{"cell_type":"code","metadata":{"id":"FwCq8pBhu6dz"},"source":["# report test accuracy\n","acc = testNetwork(model_SGD, data['X_test'], data['y_test'])\n","print(\"Test accuracy of model_SGD: {}\".format(acc))\n","# report test accuracy\n","acc = testNetwork(model_SGD_Momentum, data['X_test'], data['y_test'])\n","print(\"Test accuracy of model_SGD_Momentum: {}\".format(acc))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KQUQgfyCHzxn"},"source":["My observation:"]},{"cell_type":"markdown","metadata":{"id":"oTrmbULS7i2N"},"source":["# 4.2(f) Plot\n","\n","Using the train_acc_history and val_acc_history, plot the train & val accuracy versus epochs on one plot, using SGD and SGD_Momentum as optimizer."]},{"cell_type":"code","metadata":{"id":"SPjtnbya9S7g"},"source":["#######################################################################\n","# Your Code here\n","#######################################################################\n","\n","#######################################################################\n","#                         END OF YOUR CODE                            #\n","#######################################################################"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A7MoINIIhYf7"},"source":["# Convert Notebook to PDF"]},{"cell_type":"code","source":["# generate pdf\n","# Please provide the full path of the notebook file below\n","# Important: make sure that your file name does not contain spaces!\n","import os\n","notebookpath = '' # Ex: notebookpath = '/content/drive/My Drive/Colab Notebooks/EECS 442 Fall 2023 - PS1.ipynb'\n","drive_mount_point = '/content/drive/'\n","from google.colab import drive\n","drive.mount(drive_mount_point)\n","file_name = notebookpath.split('/')[-1]\n","get_ipython().system(\"apt update && apt install texlive-xetex texlive-fonts-recommended texlive-generic-recommended\")\n","get_ipython().system(\"pip install pypandoc\")\n","get_ipython().system(\"apt-get install texlive texlive-xetex texlive-latex-extra pandoc\")\n","get_ipython().system(\"jupyter nbconvert --to PDF {}\".format(notebookpath.replace(' ', '\\\\ ')))\n","from google.colab import files\n","files.download(notebookpath.split('.')[0]+'.pdf')"],"metadata":{"id":"ediC1vMGX1iw","executionInfo":{"status":"error","timestamp":1696375018746,"user_tz":240,"elapsed":8689,"user":{"displayName":"Yueqi Wu","userId":"16342215818375271323"}},"outputId":"63270000-b7dc-4a53-ae19-6a0b6ab2efc8","colab":{"base_uri":"https://localhost:8080/","height":394}},"execution_count":null,"outputs":[{"output_type":"error","ename":"MessageError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-22ed4d969862>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdrive_mount_point\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrive_mount_point\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnotebookpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"apt update && apt install texlive-xetex texlive-fonts-recommended texlive-generic-recommended\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    130\u001b[0m   )\n\u001b[1;32m    131\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"]}]},{"cell_type":"markdown","metadata":{"id":"d22F8kh1JyE7"},"source":["#Alternative way to convert pdf\n","If the above method does not work, please look into [this instruction](https://docs.google.com/document/d/1QTutnoApRow8cOxNrKK6ISEkA72QGfwLFXbIcpvarAI/edit?usp=sharing)."]}]}