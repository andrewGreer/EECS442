{"cells":[{"cell_type":"markdown","metadata":{"id":"ix5dQS2rUMlu"},"source":["#EECS 442 PS7: Representation learning\n","\n","__Please provide the following information__\n","(e.g. Andrew Owens, ahowens):\n","\n","[Your first name] [Your last name], [Your UMich uniqname]\n","\n","__Important__: after you download the .ipynb file, please name it as __\"PS\\<this_ps_number\\>_\\<your_uniqname\\>.ipynb\"__ before you submit it to canvas. Example: PS7_adam.ipynb.\n"]},{"cell_type":"markdown","metadata":{"id":"W_Cst4k4tuBc"},"source":["# Starting\n","\n","Run the following code to import the modules you'll need. After your finish the assignment, remember to run all cells and save the note book to your local machine as a .ipynb file for Canvas submission."]},{"cell_type":"markdown","metadata":{"id":"eIXWSou6h_S6"},"source":["### Google Colab Setup\n","We need to run a few commands to set up our environment on Google Colab. If you are running this notebook on a local machine you can skip this section.\n","\n","Run the following cell to mount your Google Drive. Follow the link, sign in to your Google account (the same account you used to store this notebook!) and copy the authorization code into the text box that appears below."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"evqEGDXRipC-"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SHumIO-xt57H"},"outputs":[],"source":["!pip install torchsummary\n","!pip install transformers\n","import pickle\n","import math\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import os\n","import cv2\n","import time\n","import itertools\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Dataset\n","\n","import torchvision\n","from torchvision import datasets, models, transforms\n","from torchsummary import summary\n","\n","import albumentations as A\n","import matplotlib.pyplot as plt\n","from transformers import DistilBertModel, DistilBertConfig, DistilBertTokenizer\n","from tqdm import tqdm_notebook\n","from tqdm.autonotebook import tqdm\n","\n","print(\"PyTorch Version: \",torch.__version__)\n","print(\"Torchvision Version: \",torchvision.__version__)\n","# Detect if we have a GPU available\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","if torch.cuda.is_available():\n","    print(\"Using the GPU!\")\n","else:\n","    print(\"WARNING: Could not find GPU! Using CPU only. If you want to enable GPU, please to go Edit > Notebook Settings > Hardware Accelerator and select GPU.\")\n","\n","np.random.seed(42)\n","torch.manual_seed(42)\n","torch.cuda.manual_seed_all(42)"]},{"cell_type":"markdown","metadata":{"id":"apEPzDNtK0MC"},"source":["# PS 7. Self-supervised learning\n","\n","In this problem, we are going to implement two representation learning methods: an autoencoder and a recent constrastive learning method.\n","We'll then test the features that were learned by these models on a \"downstream\" recognition task, using the STL-10 dataset.\n"]},{"cell_type":"markdown","metadata":{"id":"SXfumCQ21JoK"},"source":["# Downloading the dataset.\n","\n","We use PyTorch built-in class to download  the STL-10 (http://ai.stanford.edu/~acoates/stl10/) dataset (a subset of ImageNet). The STL-10 dataset contains three partitions: train, test, and unlabeled. The train partition contains 10 image classes, each class with 500 images. The test partition contains 800 images for each class. The unlabeled contains a total of 100,000 images with many classes not in the train/test partitions."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q-ljfgMv9PHx"},"outputs":[],"source":["unlabeled_transform = transforms.Compose([transforms.RandomHorizontalFlip(),\n","                                         transforms.RandomCrop(64),\n","                                         transforms.ToTensor()])\n","\n","labeled_transform = transforms.Compose([transforms.CenterCrop(64),\n","                                        transforms.ToTensor(),])\n","\n","\n","# We use the PyTorch built-in class to download the STL-10 dataset.\n","# The 'unlabeled' partition contains 100,000 images without labels.\n","# It's used for leanring representations with unsupervised learning.\n","dataset_un = torchvision.datasets.STL10('./data', 'unlabeled', download=True, transform=unlabeled_transform)\n","\n","dataset_tr = torchvision.datasets.STL10('./data', 'train', download=False, transform=labeled_transform)\n","dataset_te = torchvision.datasets.STL10('./data', 'test', download=False, transform=labeled_transform)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aGyCDpgAlXWH"},"outputs":[],"source":["print('# of samples for ulabeled, train, and test, {}, {}, {}'.format(len(dataset_un), len(dataset_tr), len(dataset_te)))\n","print('Classes in train: {}'.format(dataset_tr.classes))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hvv7CoDCh3vX"},"outputs":[],"source":["# Visualize the data within the dataset\n","class_names = dict(zip(range(10), dataset_tr.classes))\n","dataloader_un = DataLoader(dataset_un, batch_size=64)\n","dataloader_tr = DataLoader(dataset_tr, batch_size=64)\n","\n","def imshow(inp, title=None, ax=None, figsize=(5, 5)):\n","  \"\"\"Imshow for Tensor.\"\"\"\n","  inp = inp.numpy().transpose((1, 2, 0))\n","  if ax is None:\n","    fig, ax = plt.subplots(1, figsize=figsize)\n","  ax.imshow(inp)\n","  ax.set_xticks([])\n","  ax.set_yticks([])\n","  if title is not None:\n","    ax.set_title(title)\n","\n","# Visualize training partition\n","# Get a batch of training data\n","inputs, classes = next(iter(dataloader_tr))\n","\n","# Make a grid from batch\n","out = torchvision.utils.make_grid(inputs, nrow=8)\n","\n","fig, ax = plt.subplots(1, figsize=(10, 10))\n","title = [class_names[x.item()] if (i+1) % 8 != 0 else class_names[x.item()]+'\\n' for i, x in enumerate(classes)]\n","imshow(out, title=' | '.join(title), ax=ax)\n","\n","# Visualize unlabeled partition\n","inputs, classes = next(iter(dataloader_un))\n","out = torchvision.utils.make_grid(inputs, nrow=8)\n","\n","fig, ax = plt.subplots(1, figsize=(10, 10))\n","imshow(out, title='unlabeled', ax=ax)"]},{"cell_type":"markdown","metadata":{"id":"aw5Ua56tlKac"},"source":["As can be seen from above visualizations, the unlabeled partition contains classes that are not in the training partition. Though not labeled, the unlabeled partition has much more data than the labeled training partition. The large amount of unlabeled label ought to help us learn useful representations. In the next sections, we will use the unlabeled partition to help learn representations that is helpful for downstream tasks."]},{"cell_type":"markdown","metadata":{"id":"ulKktfQxiTwk"},"source":["# Part 1. Autoencoders\n","\n","We will first build an autoencoder. To keep training time low, we'll use a very simple network structure."]},{"cell_type":"markdown","metadata":{"id":"YMdS_LTgqqJh"},"source":["## 1.1 Build the encoder\n","Please make sure that your encoder has the same architeture as we print below before your proceed to the decoder part.\n","\n","### Encoder archiecture\n","\n","----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #  \n","            Conv2d-1           [-1, 12, 32, 32]             588  \n","              ReLU-2           [-1, 12, 32, 32]               0  \n","            Conv2d-3           [-1, 24, 16, 16]           4,632  \n","              ReLU-4           [-1, 24, 16, 16]               0  \n","            Conv2d-5             [-1, 48, 8, 8]          18,480  \n","              ReLU-6             [-1, 48, 8, 8]               0   \n","            Conv2d-7             [-1, 24, 4, 4]          18,456  \n","              ReLU-8             [-1, 24, 4, 4]               0  \n","Total params: 42,156  \n","Trainable params: 42,156  \n","Non-trainable params: 0  \n","Input size (MB): 0.05\n","Forward/backward pass size (MB): 0.33  \n","Params size (MB): 0.16  \n","Estimated Total Size (MB): 0.54  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wNskeJbxkH-o"},"outputs":[],"source":["class Encoder(nn.Module):\n","    def __init__(self, in_channels=3):\n","        super(Encoder, self).__init__()\n","\n","        ##############################################################################\n","        #                               YOUR CODE HERE                               #\n","        ##############################################################################\n","        # TODO: Build an encoder with the architecture as specified above.           #\n","        ##############################################################################\n","        pass\n","        ##############################################################################\n","        #                               END OF YOUR CODE                             #\n","        ##############################################################################\n","\n","    def forward(self, x):\n","        '''\n","        Given an image x, return the encoded latent representation h.\n","\n","        Args:\n","            x: torch.tensor\n","\n","        Return:\n","            h: torch.tensor\n","        '''\n","\n","        h = self.encoder(x)\n","\n","        return h"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H5sPqIQQnc-R"},"outputs":[],"source":["# Print out the neural network architectures and activation dimensions.\n","# Verify that your network has the same architecture as the one we printed above.\n","encoder = Encoder().to(device)\n","summary(encoder, [(3, 64, 64)])"]},{"cell_type":"markdown","metadata":{"id":"lsboRJ7SvqOb"},"source":["## 1.2 Build the decoder\n","\n","Next, we build the decoder to reconstruct the image from the latent representation extracted by the encoder. Please implement the decoder following the architectrue printed here."]},{"cell_type":"markdown","metadata":{"id":"71zf8LV65-7X"},"source":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #  \n","              ConvTranspose2d-1   [-1, 48, 8, 8]         18,480  \n","              ReLU-2              [-1, 48, 8, 8]              0  \n","              ConvTranspose2d-3   [-1, 24, 16, 16]       18,456  \n","              ReLU-4              [-1, 24, 16, 16]            0\n","              ConvTranspose2d-5   [-1, 12, 32, 32]        4,620  \n","              ReLU-6              [-1, 12, 32, 32]            0  \n","              ConvTranspose2d-7   [-1, 3, 64, 64]           579  \n","              Sigmoid-8           [-1, 3, 64, 64]             0  \n","\n","Total params: 42,135  \n","Trainable params: 42,135  \n","Non-trainable params: 0  \n","Input size (MB): 0.00  \n","Forward/backward pass size (MB): 0.52  \n","Params size (MB): 0.16  \n","Estimated Total Size (MB): 0.68  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"08CKj2ZAvoxy"},"outputs":[],"source":["class Decoder(nn.Module):\n","    def __init__(self, out_channels=3, feat_dim=64):\n","        super(Decoder, self).__init__()\n","\n","        ##############################################################################\n","        #                               YOUR CODE HERE                               #\n","        ##############################################################################\n","        # TODO: Build the decoder as specified above.                                #\n","        ##############################################################################\n","        pass\n","        ##############################################################################\n","        #                               END OF YOUR CODE                             #\n","        ##############################################################################\n","\n","    def forward(self, h):\n","        '''\n","        Given latent representation h, reconstruct an image patch of size 64 x 64.\n","\n","        Args:\n","            h: torch.tensor\n","\n","        Return:\n","            x: torch.tensor\n","        '''\n","        x = self.decoder(h)\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2ksNu0aiyJ6C"},"outputs":[],"source":["# Print out the neural network architectures and activation dimensions.\n","# Verify that your network has the same architecture as the one we printed above.\n","decoder = Decoder().to(device)\n","summary(decoder, [(24, 4, 4)])"]},{"cell_type":"markdown","metadata":{"id":"2AXqgRK77TxF"},"source":["## 1.3 Put together the autoencoder\n","\n","Now we have the encoder and the decoder classes. We only need to implement another `Autoencoder` class to wrap the encoder and the decoder together."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EdDlHSPK7S7S"},"outputs":[],"source":["class Autoencoder(nn.Module):\n","    def __init__(self, in_channels=3, feat_dim=64):\n","        super(Autoencoder, self).__init__()\n","\n","        self.encoder = Encoder()\n","        self.decoder = Decoder()\n","\n","    def forward(self, x):\n","        '''\n","        Compress and reconstruct the input image with encoder and decoder.\n","\n","        Args:\n","            x: torch.tensor\n","\n","        Return:\n","            x_: torch.tensor\n","        '''\n","\n","        h = self.encoder(x)\n","        x_ = self.decoder(h)\n","\n","        return x_"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UINdckEH84mc"},"outputs":[],"source":["# verify that your aueconder's output size is 3 x 64 x 64\n","ae = Autoencoder().to(device)\n","summary(ae, (3, 64, 64))"]},{"cell_type":"markdown","metadata":{"id":"YGkLBQEC7JU9"},"source":["## 1.4 Training the autoencoder\n","\n","Now, we'll train the autoencoder to reconstruct images from the unlabeled set of STL-10. Note that the reconstructed images will contain significant artifacts, due to the limited size of the bottleneck between the encoder and decoder, and the small network size."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xDF5gF-Z-Pze"},"outputs":[],"source":["# We train on 10,000 unsupervised samples instead of 100,000 samples to speed up training\n","n = 10000\n","dataset_un_subset, _ = torch.utils.data.random_split(dataset_un, [n,100000-n])\n","dataloader_un = DataLoader(dataset_un_subset, batch_size=128, shuffle=True)\n","dataloader_tr = DataLoader(dataset_tr, batch_size=128, shuffle=True)\n","dataloader_te = DataLoader(dataset_te, batch_size=128, shuffle=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mcTfYNi-WrwY"},"outputs":[],"source":["def visualize_recon(model, dataloader):\n","    '''\n","    Helper function for visualizing reconstruction performance.\n","\n","    Randomly sample 8 images and plot the original/reconstructed images.\n","    '''\n","    model.eval()\n","    img = next(iter(dataloader))[0][:8].to(device)\n","    out = model(img)\n","\n","    fig, ax = plt.subplots(1, 1, figsize=(15,10))\n","    inp = torchvision.utils.make_grid(torch.cat((img, out), dim=2), nrow=8)\n","    imshow(inp.detach().cpu(), ax=ax)\n","    model.train()\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hgmm4lWe9hJi"},"outputs":[],"source":["def train_ae(model, dataloader, epochs=200):\n","    '''\n","    Train autoencoder model.\n","\n","    Args:\n","        model: torch.nn.module.\n","        dataloader: DataLoader. The unlabeled partition of the STL dataset.\n","    '''\n","\n","    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n","    criterion = nn.MSELoss()\n","    loss_traj = []\n","\n","    for epoch in tqdm_notebook(range(epochs)):\n","\n","        loss_epoch = 0\n","        for x, _ in dataloader:\n","\n","            ##############################################################################\n","            #                               YOUR CODE HERE                               #\n","            ##############################################################################\n","            # TODO: Train the autoencoder on one minibatch.                              #\n","            ##############################################################################\n","            pass\n","            ##############################################################################\n","            #                               END OF YOUR CODE                             #\n","            ##############################################################################\n","            loss_epoch += loss.detach()\n","\n","        loss_traj.append(loss_epoch)\n","\n","        if epoch % 10 == 0:\n","            print('Epoch {}, loss {:.3f}'.format(epoch, loss_epoch))\n","            visualize_recon(model, dataloader)\n","\n","    return model, loss_traj"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NqLE-MovANH4"},"outputs":[],"source":["# Train the autoencoder for 100 epochs\n","ae = Autoencoder().to(device)\n","ae, ae_loss_traj = train_ae(ae, dataloader_un, epochs=100)\n","torch.save(ae.state_dict(), 'ae.pth')"]},{"cell_type":"markdown","metadata":{"id":"nAzm6q1zaYpV"},"source":["After training the autoencoder on the 100,000 images for 100 epochs, we see that autoencoder has leanred to approximately recontruct the image."]},{"cell_type":"markdown","metadata":{"id":"EbyxnfnhbjTk"},"source":["## 1.5 Train a linear classifier\n","\n","Now, we ask how useful the features are for object recongition. We'll train a linear clasifier that takes the output of the encoder as its features. During training, we freeze the parameters of the encoder. To verify the effectiveness of unsupervised pretraining, we compare the linear classifier accuracy against two baselines:\n","\n","* Supervised: train the encoder together with the linear classifier on the training set for 100 epochs.\n","* Random weights: freeze the parameters of a randomly initialized encoder during training."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j-zOQezRpvBe"},"outputs":[],"source":["# latent representation dimension (the output dimension of the encoder)\n","feat_dim = 24 * 4 * 4"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mjmAZc_lbh6G"},"outputs":[],"source":["def train_classifier(encoder, cls, dataloader, epochs=100, supervised=False):\n","    '''\n","    Args:\n","        encoder: trained/untrained encoder for unsupervised/supervised training.\n","        cls: linear classifier.\n","        dataloader: train partition.\n","        supervised:\n","\n","    Return:\n","        cls: linear clssifier.\n","    '''\n","\n","    optimizer = optim.Adam(cls.parameters(), lr=0.001, weight_decay=1e-4)\n","    if supervised:\n","        optimizer = optim.Adam(list(cls.parameters())+list(encoder.parameters()), lr=0.001, weight_decay=1e-4)\n","    criterion = nn.CrossEntropyLoss()\n","    loss_traj = []\n","    accuracy_traj = []\n","\n","    for epoch in tqdm_notebook(range(epochs)):\n","\n","        loss_epoch = 0\n","        corrects_epoch = 0\n","        for x, y in dataloader:\n","\n","            batch_size = x.size(0)\n","            x = x.float()\n","            ##############################################################################\n","            #                               YOUR CODE HERE                               #\n","            ##############################################################################\n","            # TODO: update the parameters of the classifer. If in supervised mode, the   #\n","            # parameter of the encoder is also updated.                                  #\n","            ##############################################################################\n","            pass\n","            ##############################################################################\n","            #                               END OF YOUR CODE                             #\n","            ##############################################################################\n","            _, preds = torch.max(outs, 1)\n","            corrects_epoch += torch.sum(preds == y.data)\n","            loss_epoch += loss.detach()\n","\n","        loss_traj.append(loss_epoch)\n","        epoch_acc = corrects_epoch.double() / len(dataloader.dataset)\n","        accuracy_traj.append(epoch_acc)\n","\n","        if epoch % 10 == 0:\n","            print('Epoch {}, loss {:.3f}, train accuracy {}'.format(epoch, loss_epoch, epoch_acc))\n","\n","    return cls, loss_traj"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JhS9Pl8vk9lK"},"outputs":[],"source":["def test(encoder, cls, dataloader):\n","    '''\n","    Calculate the accuracy of the trained linear classifier on the test set.\n","    '''\n","    cls.eval()\n","\n","    loss_epoch = 0\n","    corrects_epoch = 0\n","    for x, y in dataloader:\n","\n","        x = x.float()\n","        batch_size = x.size(0)\n","        x, y = x.to(device), y.to(device)\n","        h = encoder(x).view(batch_size, -1)\n","        outs = cls(h)\n","        _, preds = torch.max(outs, 1)\n","        corrects_epoch += torch.sum(preds == y.data)\n","\n","    epoch_acc = corrects_epoch.double() / len(dataloader.dataset)\n","    print('Test accuracy {}'.format(epoch_acc))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TLsEtf-wdMoK"},"outputs":[],"source":["# Method I: unsupervised pretraining + training linear classifier\n","# Freeze the parameters of the trained autoencoder\n","# Train a linear classifier using the features\n","# TODO: set the supervised parameter\n","linear_cls = nn.Sequential(nn.Linear(feat_dim, 10)).to(device)\n","cls_unsupervised, loss_traj_unsupervised = train_classifier(ae.encoder, linear_cls, dataloader_tr, epochs=100, supervised=)\n","test(ae.encoder, cls_unsupervised, dataloader_te)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NBWwX7jwkBc9"},"outputs":[],"source":["# Method II: supervised training\n","# Train the encoder together with the linear classifier\n","# TODO: set the supervised parameter\n","linear_cls = nn.Sequential(nn.Linear(feat_dim, 10)).to(device)\n","encoder = Autoencoder().to(device).encoder\n","cls_supervised, loss_traj_supervised = train_classifier(encoder, linear_cls, dataloader_tr, epochs=100, supervised=)\n","test(encoder, cls_supervised, dataloader_te)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-hNV69crBli1"},"outputs":[],"source":["# Method III: random encoder + training linear classifier\n","# We freeze the parameters of the randomly initialized encoder during training\n","# TODO: set the supervised parameter\n","linear_cls = nn.Sequential(nn.Linear(feat_dim, 10)).to(device)\n","encoder = Autoencoder().to(device).encoder\n","cls_random, loss_traj_random = train_classifier(encoder, linear_cls, dataloader_tr, epochs=100, supervised=)\n","test(encoder, cls_random, dataloader_te)"]},{"cell_type":"markdown","metadata":{"id":"6U4R9z4qf9jj"},"source":["With pretrained encoder, the linear classifier should achieve about 30% accuracy on the test set. With the supervised approach, the linear classifier should achieve an accuracy above 40%. The random encoder approach performs the worse among these three. The observation that the pretrained encoder outperforms the random encoder confirms that unsupervised pretraining has learned a useful represenation. However, the quality of this learned representation is not good because it only performs slightly better than the random encoder. In the next part, we'll explore contrastive multiview coding, which learns a more useful representation than the autoencoder."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4VRsu2tBa5S8"},"outputs":[],"source":["del dataset_un\n","del dataset_te\n","del dataset_tr"]},{"cell_type":"markdown","metadata":{"id":"AAwTa1xEjgPK"},"source":["# Part 2. Contrastive Language-Image Pre-training (CLIP)\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"AAi0zsoZbJFd"},"source":["In this part, we will implement a simplified version of the [Contrastive Language-Image Pre-training (CLIP)](https://arxiv.org/abs/2103.00020). This is a variation of the contrastive representation learning method we discussed in class. In a sentence, this model learns the relationship between a natural language sentence and the image this sentence describes. We'll learn a vector representation for images and a vector representation for natural language texts. Learning directly from raw texts gives us a broader source of supervision compared to the standard crowd-sourced labeling for image classification. It also has an advantage over most unsupervised or self-supervised learning approaches, since it doesn't just learn a representation, but also connects the representation to the natural language.\n","\n","<!-- ![alt text](https://drive.google.com/uc?id=1RHmYiXeXq7vPR1MpLVE8xfqkb7jjv6Cg) -->\n","\n","![alt text](https://drive.google.com/uc?id=1ZVffv0C_DcWEWnqmi_n8c1KHDWLePUWH)\n","\n","\n","The core idea behind CLIP is that, when an image matches with a natural language sentence, their vector representations should have a large dot product. Whereas, the dot product between the representations of an image and a text that are not related to each other should be small.\n","\n"]},{"cell_type":"markdown","source":["## Setup\n","As the authors suggest, in CLIP the dataset plays a major role for the performance and we need both image and texts for training the CLIP model. We will use [flickr8k](https://www.kaggle.com/datasets/adityajn105/flickr8k) to train our model. This is a small dataset with 8000 images, which would run well given the limited comuptation units on Colab. Each image in the dataset is paired with 5 captions, we will just use one of the 5 captions for simplicity. Below we displayed some sample captions corresponding to an image.\n"],"metadata":{"id":"K0yMl5Q5oCdv"}},{"cell_type":"code","source":["!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1csewExbHtYIcOr8BWMrGd8DqX_awmP1t' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1csewExbHtYIcOr8BWMrGd8DqX_awmP1t\" -O flickr8k.zip && rm -rf /tmp/cookies.txt\n","!unzip flickr8k.zip"],"metadata":{"id":"b8wHh7EHwOIt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# dataset pre-processing\n","df = pd.read_csv(\"captions.txt\")\n","img_color = cv2.imread('/content/Images/'+ str(df['image'][0]),1)\n","plt.imshow(cv2.cvtColor(img_color, cv2.COLOR_BGR2RGB))\n","df['id'] = [id_ for id_ in range(df.shape[0] // 5) for _ in range(5)]\n","df.to_csv(\"captions.csv\", index=False)\n","df = pd.read_csv(\"captions.csv\")\n","df.head()\n"],"metadata":{"id":"KHeNSQo6pFmq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Here are some configuration variables, hyperparameter and utility functions that we will be using through this section:"],"metadata":{"id":"7kouJ-uHqlyn"}},{"cell_type":"code","source":["image_path = \"/content/Images\"\n","captions_path = \"/content\"\n","batch_size = 32\n","num_workers = 2\n","head_lr = 1e-3\n","image_encoder_lr = 1e-4\n","text_encoder_lr = 1e-5\n","weight_decay = 1e-3\n","patience = 1\n","factor = 0.8\n","epochs = 3\n","\n","image_encoder_model = 'resnet50'\n","image_embedding = 2048\n","text_encoder_model = \"distilbert-base-uncased\"\n","text_embedding = 768\n","text_tokenizer = \"distilbert-base-uncased\"\n","max_length = 200\n","\n","pretrained = True # for both image encoder and text encoder\n","trainable = True # for both image encoder and text encoder\n","temperature = 0.07\n","\n","image_size = 64\n","\n","# for projection head; used for both image and text encoders\n","projection_dim = 256\n","\n","class AvgMeter:\n","    def __init__(self, name=\"Metric\"):\n","        self.name = name\n","        self.reset()\n","\n","    def reset(self):\n","        self.avg, self.sum, self.count = [0] * 3\n","\n","    def update(self, val, count=1):\n","        self.count += count\n","        self.sum += val * count\n","        self.avg = self.sum / self.count\n","\n","    def __repr__(self):\n","        text = f\"{self.name}: {self.avg:.4f}\"\n","        return text\n","\n","def get_lr(optimizer):\n","    for param_group in optimizer.param_groups:\n","        return param_group[\"lr\"]"],"metadata":{"id":"VTasHvH2qs_D"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Dataset\n","\n","Below we are writing the custom dataloader that can handle both the images and texts necessary to train the CLIP model. It can be noted from the CLIP figure that, we need to encode both images and their describing texts. Since, we are not going to feed raw text to our text encoder! We will use the [DistilBERT](https://huggingface.co/docs/transformers/model_doc/distilbert) (which is smaller than BERT but performs nearly as well as BERT) model as our text encoder. Before that, we need to tokenize the sentences (captions) with DistilBERT tokenizer and then feed the token ids (input_ids) and the attention masks to DistilBERT. We have implemented the tokenization scheme and you do not have to implement anything for this custom dataloader.\n"],"metadata":{"id":"fGZS6hzvrgIK"}},{"cell_type":"code","source":["class CLIPDataset(torch.utils.data.Dataset):\n","    def __init__(self, image_filenames, captions, tokenizer, transforms):\n","        \"\"\"\n","        image_filenames and cpations must have the same length; so, if there are\n","        multiple captions for each image, the image_filenames must have repetitive\n","        file names\n","        \"\"\"\n","\n","        self.image_filenames = image_filenames\n","        self.captions = list(captions)\n","        self.encoded_captions = tokenizer(\n","            list(captions), padding=True, truncation=True, max_length=max_length\n","        )\n","        self.transforms = transforms\n","\n","    def __getitem__(self, idx):\n","        item = {\n","            key: torch.tensor(values[idx])\n","            for key, values in self.encoded_captions.items()\n","        }\n","\n","        image = cv2.imread(f\"{image_path}/{self.image_filenames[idx]}\")\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","        image = self.transforms(image=image)['image']\n","        item['image'] = torch.tensor(image).permute(2, 0, 1).float()\n","        item['caption'] = self.captions[idx]\n","\n","        return item\n","\n","\n","    def __len__(self):\n","        return len(self.captions)\n","\n","# data augmentation for images\n","def get_transforms(mode=\"train\"):\n","    return A.Compose(\n","            [\n","                A.Resize(image_size, image_size, always_apply=True),\n","                A.Normalize(max_pixel_value=255.0, always_apply=True),\n","            ]\n","        )"],"metadata":{"id":"m2IUWxZgswI-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Image Encoder\n","We use [ResNet50](https://pytorch.org/vision/main/models/generated/torchvision.models.resnet50.html) as our image encoder. It encodes each image to a fixed size vector of the model's output channels (in the case of ResNet50, it's 2048). We will load the pre-trained ImageNet weights to the ResNet50 model to get faster convergence and the limitation on the dataset size and computation resources."],"metadata":{"id":"lTpgpRz9tSw_"}},{"cell_type":"code","source":["from torchvision.models.resnet import ResNet50_Weights\n","class ImageEncoder(nn.Module):\n","    \"\"\"\n","    Encode images to a fixed size vector\n","    \"\"\"\n","\n","    def __init__(\n","        self, model_name=image_encoder_model, pretrained=pretrained, trainable=trainable\n","    ):\n","        super().__init__()\n","        self.model = torchvision.models.resnet50(ResNet50_Weights.IMAGENET1K_V2)\n","        self.model.fc = nn.Identity()\n","\n","        for p in self.model.parameters():\n","            p.requires_grad = trainable\n","\n","    def forward(self, x):\n","        return self.model(x)"],"metadata":{"id":"i-y5R_8nt_HP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Text Encoder\n","As mentioned above, we use DistilBERT as our text encoder. The output representation is a vector of size 768."],"metadata":{"id":"ww7Z0NyouDrN"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"am5VR4Ezi6lZ"},"outputs":[],"source":["class TextEncoder(nn.Module):\n","    def __init__(self, model_name=text_encoder_model, pretrained=pretrained, trainable=trainable):\n","        super().__init__()\n","        if pretrained:\n","            self.model = DistilBertModel.from_pretrained(model_name)\n","        else:\n","            self.model = DistilBertModel(config=DistilBertConfig())\n","\n","        for p in self.model.parameters():\n","            p.requires_grad = trainable\n","\n","        # we are using the CLS token hidden representation as the sentence's embedding\n","        self.target_token_idx = 0\n","\n","    def forward(self, input_ids, attention_mask):\n","        output = self.model(input_ids=input_ids, attention_mask=attention_mask)\n","        last_hidden_state = output.last_hidden_state\n","        return last_hidden_state[:, self.target_token_idx, :]"]},{"cell_type":"markdown","source":["## Projection Head\n","\n","Now that we have encoded both our images and texts into fixed size vectors (2048 for image and 768 for text), we need to project both embeddings into a new vector space (!) with similar dimensions (256). This allows for both images and texts to be able to compare them and push apart the non-relevant image and texts and pull together those that match."],"metadata":{"id":"BdSqW4vBuuJo"}},{"cell_type":"code","source":["class ProjectionHead(nn.Module):\n","    def __init__(\n","        self,\n","        embedding_dim,\n","        projection_dim=projection_dim,\n","    ):\n","        super().__init__()\n","        '''\n","        Args:\n","            embedding_dim (int): Extracted Image or text feature embedding dimenasion.\n","        '''\n","\n","        ##############################################################################\n","        #                               YOUR CODE HERE                               #\n","        ##############################################################################\n","        # TODO: Initialize a single layer linear transformation for the projection   #\n","        # head.                                                                      #\n","        ##############################################################################\n","        pass\n","        ##############################################################################\n","        #                               END OF YOUR CODE                             #\n","        ##############################################################################\n","\n","    def forward(self, x):\n","        '''\n","        Args:\n","            x: Image or text feature embeddings extracted from the ResNet50 and DistilBERT model respectively.\n","\n","        Return:\n","            projected: The projected image and text embeddings.\n","        '''\n","        ##############################################################################\n","        #                               YOUR CODE HERE                               #\n","        ##############################################################################\n","        # TODO: Write the forward function. Normalize the output of the projection   #\n","        # head. Hint: use F.normalize() for the normalization                        #\n","        ##############################################################################\n","\n","        pass\n","\n","        ##############################################################################\n","        #                               END OF YOUR CODE                             #\n","        ##############################################################################\n","        return projected"],"metadata":{"id":"tTLYNLcJuti0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## CLIP\n","In this section, you will need to use the modules that you just implemented to build the CLIP model. The loss function we minimize is defined as:\n","\n","\n","$$\n","\\mathcal{L}_{\\text {contrast }}^{I, T}=-\\frac{1}{N}\\sum_{i=1}^N \\log \\frac{\\exp(I_{i}\\cdot T_{i}/\\tau)}{\\sum_{j=1}^{N} \\exp(I_{i}\\cdot T_{j}/\\tau)},\n","$$\n","\n","\n","where $I_i$ and $T_i$ are the image feature and its corresponding text representations for the $i_{th}$ sample in a minibatch, and $N$ denotes the batch size. The constant $\\tau$ is used to control the range of logits.\n","\n","\n","\n","We will minimize a symmetric objective function that averages $\\mathcal{L}_{\\text {contrast }}^{I, T}$ and $\\mathcal{L}_{\\text {contrast }}^{T, I}$, i.e.,\n","\n","\n","\n","$$\\mathcal{L}=\\frac{\\mathcal{L}_{\\text {contrast }}^{I, T}+\\mathcal{L}_{\\text {contrast }}^{T, I}}{2},\n","$$\n","\n","where,\n","$$% \\mathcal{L}_{\\text {contrast }}^{I, T}=-\\frac{1}{N}\\sum_{i=1}^N \\log \\frac{\\exp(I_{i}\\cdot T_{i})}{\\sum_{j=1}^{N} \\exp(I_{i}\\cdot T_{j})}\\cdot\\exp(\\tau),\n","\\mathcal{L}_{\\text {contrast }}^{T, I}=-\\frac{1}{N}\\sum_{i=1}^N \\log \\frac{\\exp(T_{i}\\cdot I_{i}/\\tau)}{\\sum_{j=1}^{N} \\exp(T_{i}\\cdot I_{j}/\\tau)},\n","$$\n","\n","By minimizing the above loss function, we learn representations for the image and text such that the positive pairs (respective images and their captions) will produce high dot product while giving low dot product for negative samples (image and other unrelated captions).\n","\n"],"metadata":{"id":"3cprgIJrvsOL"}},{"cell_type":"code","source":["class CLIPModel(nn.Module):\n","    def __init__(\n","        self,\n","        temperature=temperature,\n","        image_embedding=image_embedding,\n","        text_embedding=text_embedding,\n","    ):\n","        super().__init__()\n","        '''\n","        Args:\n","            temperature (float): temperature parameter which controls the range of the logits.\n","            image_embedding (int): Shape of the extracted image embedding\n","            text_embedding (int): Shape of the extracted text embedding\n","\n","        '''\n","\n","        self.image_encoder = None\n","        self.text_encoder = None\n","        self.image_projection = None\n","        self.text_projection = None\n","        self.temperature = temperature\n","        ##############################################################################\n","        #                               YOUR CODE HERE                               #\n","        ##############################################################################\n","        # TODO: Initialize the encoders and the projection heads for image and text i.e,\n","        # instantiate the above None variables with their corresponding models       #\n","        ##############################################################################\n","        pass\n","        ##############################################################################\n","        #                               END OF YOUR CODE                             #\n","        ##############################################################################\n","\n","\n","    def forward(self, batch):\n","\n","        '''\n","        Args:\n","            batch: batch of images for training.\n","\n","        Return:\n","            loss: computed loss.\n","        '''\n","        # get image and text features\n","        image_features = self.image_encoder(batch[\"image\"])\n","        text_features = self.text_encoder(\n","            input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]\n","        )\n","\n","        loss = None\n","        ##############################################################################\n","        #                               YOUR CODE HERE                               #\n","        ##############################################################################\n","        # TODO: Project image_features and text_features into a new vector space and write the loss function by following the above equations\n","        # Hint: You can make use of nn.CrossEntropyLoss() or nn.LogSoftmax() when calculating the loss\n","        # you are not allowed to use any for loops while computing the loss.\n","        ##############################################################################\n","        pass\n","        ##############################################################################\n","        #                               END OF YOUR CODE                             #\n","        ##############################################################################\n","\n","        return loss.mean()\n"],"metadata":{"id":"74l294eGzhex"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Training\n","Next we will need to train the CLIP model and here are some utility functions that are necessary to train."],"metadata":{"id":"PO9f9Nivz8Cm"}},{"cell_type":"code","source":["def make_train_valid_dfs():\n","    dataframe = pd.read_csv(f\"{captions_path}/captions.csv\")\n","    dataframe = dataframe[dataframe.reset_index().index % 5 == 0]\n","    max_id = dataframe[\"id\"].max() + 1\n","    image_ids = np.arange(0, max_id)\n","    np.random.seed(42)\n","    valid_ids = np.random.choice(\n","        image_ids, size=int(0.2 * len(image_ids)), replace=False\n","    )\n","    train_ids = [id_ for id_ in image_ids if id_ not in valid_ids]\n","    train_dataframe = dataframe[dataframe[\"id\"].isin(train_ids)].reset_index(drop=True)\n","    valid_dataframe = dataframe[dataframe[\"id\"].isin(valid_ids)].reset_index(drop=True)\n","    return train_dataframe, valid_dataframe\n","\n","\n","def build_loaders(dataframe, tokenizer, mode):\n","    transforms = get_transforms(mode=mode)\n","    dataset = CLIPDataset(\n","        dataframe[\"image\"].values,\n","        dataframe[\"caption\"].values,\n","        tokenizer=tokenizer,\n","        transforms=transforms,\n","    )\n","    dataloader = torch.utils.data.DataLoader(\n","        dataset,\n","        batch_size=batch_size,\n","        num_workers=num_workers,\n","        shuffle=True if mode == \"train\" else False,\n","        drop_last=True\n","    )\n","    return dataloader\n","\n","def train_epoch(model, train_loader, optimizer, lr_scheduler, step):\n","    loss_meter = AvgMeter()\n","    tqdm_object = tqdm(train_loader, total=len(train_loader))\n","    for i, batch in enumerate(tqdm_object):\n","        batch = {k: v.to(device) for k, v in batch.items() if k != \"caption\"}\n","        loss = model(batch)\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        if step == \"batch\":\n","            lr_scheduler.step()\n","\n","        count = batch[\"image\"].size(0)\n","        loss_meter.update(loss.item(), count)\n","\n","        tqdm_object.set_postfix(train_loss=loss_meter.avg, lr=get_lr(optimizer))\n","\n","        if i % 100 == 0:\n","            print('loss:', loss.item() / count)\n","    return loss_meter\n","\n","\n","def valid_epoch(model, valid_loader):\n","    loss_meter = AvgMeter()\n","\n","    tqdm_object = tqdm(valid_loader, total=len(valid_loader))\n","    for batch in tqdm_object:\n","        batch = {k: v.to(device) for k, v in batch.items() if k != \"caption\"}\n","        loss = model(batch)\n","\n","        count = batch[\"image\"].size(0)\n","        loss_meter.update(loss.item(), count)\n","\n","        tqdm_object.set_postfix(valid_loss=loss_meter.avg)\n","    return loss_meter"],"metadata":{"id":"gG4zc7mSz7cS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Run this cell to train the model."],"metadata":{"id":"WTmFm30x0l5L"}},{"cell_type":"code","source":["train_df, valid_df = make_train_valid_dfs()\n","tokenizer = DistilBertTokenizer.from_pretrained(text_tokenizer)\n","train_loader = build_loaders(train_df, tokenizer, mode=\"train\")\n","valid_loader = build_loaders(valid_df, tokenizer, mode=\"valid\")\n","\n","\n","model = CLIPModel().to(device)\n","params = [\n","    {\"params\": model.image_encoder.parameters(), \"lr\": image_encoder_lr},\n","    {\"params\": model.text_encoder.parameters(), \"lr\": text_encoder_lr},\n","    {\"params\": itertools.chain(\n","        model.image_projection.parameters(), model.text_projection.parameters()\n","    ), \"lr\": head_lr, \"weight_decay\": weight_decay}\n","]\n","optimizer = torch.optim.AdamW(params, weight_decay=0.)\n","lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n","    optimizer, mode=\"min\", patience=patience, factor=factor\n",")\n","step = \"epoch\"\n","\n","best_loss = float('inf')\n","for epoch in range(epochs):\n","    print(f\"Epoch: {epoch + 1}\")\n","    model.train()\n","    train_loss = train_epoch(model, train_loader, optimizer, lr_scheduler, step)\n","    model.eval()\n","    with torch.no_grad():\n","        valid_loss = valid_epoch(model, valid_loader)\n","\n","    if valid_loss.avg < best_loss:\n","        best_loss = valid_loss.avg\n","        torch.save(model.state_dict(),  \"best.pt\")\n","        print(\"Saved Best Model!\")\n","\n","    lr_scheduler.step(valid_loss.avg)"],"metadata":{"id":"S0RhoQRy0qGg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Train a Linear Classifier\n","We'll train a linear clasifier that takes the output of the encoder (Image Encoder: ResNet50) as its features. During training, we freeze the parameters of the encoder and only train the linear layer."],"metadata":{"id":"TQjFeI0l2dTR"}},{"cell_type":"code","source":["labeled_transform = transforms.Compose([\n","    transforms.CenterCrop(64),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n","    ])\n","\n","dataset_tr = torchvision.datasets.STL10('./data', 'train', download=False, transform=labeled_transform)\n","dataset_te = torchvision.datasets.STL10('./data', 'test', download=False, transform=labeled_transform)\n","dataloader_tr = DataLoader(dataset_tr, batch_size=128, shuffle=True, num_workers=2)\n","dataloader_te = DataLoader(dataset_te, batch_size=128, shuffle=False, num_workers=2)\n","model = CLIPModel().to(device)\n","model.load_state_dict(torch.load(\"best.pt\", map_location=device))\n","model.eval()\n","encoder = model.image_encoder\n","\n","linear_cls = nn.Sequential(\n","    nn.Linear(image_embedding, 10)\n","    ).to(device)\n","\n","cls_clip, loss_traj_clip = train_classifier(encoder, linear_cls, dataloader_tr, epochs=100, supervised=False)\n","test(encoder, cls_clip, dataloader_te)"],"metadata":{"id":"uYZ7z3oa2c5g"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["With pretrained encoder, the linear classifier should achieve an accuracy > 75% on the test set. Notice that the supervised flag is set to 'False' which means we are updating the weights of the final linear layer."],"metadata":{"id":"F6mEN1pYjyEl"}},{"cell_type":"markdown","source":["## CLIP Inference\n","Now we are doing a interesting downstream task to see what CLIP has learned. We will give the model a piece of text, and let the model retrieve the most relevant images from the dataset.\n","\n","First, we need to project all the images into a common search space.\n"],"metadata":{"id":"em0IaeEf3M0W"}},{"cell_type":"code","source":["def get_image_embeddings(df, model_path):\n","    tokenizer = DistilBertTokenizer.from_pretrained(text_tokenizer)\n","    dataloader = build_loaders(df, tokenizer, mode=\"valid\")\n","\n","    model = CLIPModel().to(device)\n","    model.load_state_dict(torch.load(model_path, map_location=device))\n","    model.eval()\n","\n","    image_embeddings = []\n","    with torch.no_grad():\n","        for batch in tqdm(dataloader):\n","            image_features = model.image_encoder(batch[\"image\"].to(device))\n","            image_embedding = model.image_projection(image_features)\n","            image_embeddings.append(image_embedding)\n","    return model, torch.cat(image_embeddings)\n","\n","df, _ = make_train_valid_dfs()\n","model, image_embeddings = get_image_embeddings(df, \"best.pt\")"],"metadata":{"id":"WmW1VKWp4Lv8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In this function `retrieve_images`, we will get the model, image embeddings and a text query. You need to calculate the similarities between the text and all the images and return the best `k` of them."],"metadata":{"id":"C6hmDowk4zrf"}},{"cell_type":"code","source":["def retrieve_images(model, image_embeddings, query, image_filenames, k=9):\n","    tokenizer = DistilBertTokenizer.from_pretrained(text_tokenizer)\n","    encoded_query = tokenizer([query])\n","    batch = {\n","        key: torch.tensor(values).to(device)\n","        for key, values in encoded_query.items()\n","    }\n","    with torch.no_grad():\n","        text_features = model.text_encoder(\n","            input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]\n","        )\n","        text_embeddings = model.text_projection(text_features)\n","\n","\n","    ##############################################################################\n","    #                               YOUR CODE HERE                               #\n","    ##############################################################################\n","    # TODO: Please normalize the image_embeddings and text_embeddings using L2norm,\n","    # calculate the similarity,\n","    # and then get the top k similar images to the given text query and pass them to the 'matches' variable.\n","    # Note: You cannot use any for loops in this function.\n","    ##############################################################################\n","    matches = None # Please keep this as the variable name for the matches\n","    pass\n","    ##############################################################################\n","    #                               END OF YOUR CODE                             #\n","    ##############################################################################\n","\n","    _, axes = plt.subplots(3, 3, figsize=(10, 10))\n","    for match, ax in zip(matches, axes.flatten()):\n","        image = cv2.imread(f\"{image_path}/{match}\")\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","        ax.imshow(image)\n","        ax.axis(\"off\")\n","\n","    plt.show()"],"metadata":{"id":"u9lOONec5a2a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we can query the model and get some results!"],"metadata":{"id":"EfcpDuqU5shG"}},{"cell_type":"code","source":["retrieve_images(model,\n","             image_embeddings,\n","             query=\"dogs in park\",\n","             image_filenames=train_df['image'].values)"],"metadata":{"id":"MRQhVSYQ5sFE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["retrieve_images(model,\n","             image_embeddings,\n","             query=\"dogs on grass\",\n","             image_filenames=train_df['image'].values)"],"metadata":{"id":"_pfhJrPVLGSB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["retrieve_images(model,\n","             image_embeddings,\n","             query=\"dogs running\",\n","             image_filenames=train_df['image'].values)"],"metadata":{"id":"ud0hUybTLgsi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["retrieve_images(model,\n","             image_embeddings,\n","             query=\"humans dancing and singing\",\n","             image_filenames=train_df['image'].values)"],"metadata":{"id":"CwH2nnIoLHcD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["retrieve_images(model,\n","             image_embeddings,\n","             query=\"humans doing outdoor activities\",\n","             image_filenames=train_df['image'].values)"],"metadata":{"id":"CM7cWhIKLHqW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["retrieve_images(model,\n","             image_embeddings,\n","             query=\"humans riding bikes\",\n","             image_filenames=train_df['image'].values)"],"metadata":{"id":"AGYN9shLLXHa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Report results\n","\n","So far, we have trained linear classifiers on top of autoencoder representations and CLIP representations.\n","\n","With CLIP, we learn a more useful representation than autoencoder\n","\n","Please report the test accuracy of all four linear classifiers below.\n","\n","### Autoencoder-based\n","method 1 accuracy:   \n","method 2 accuracy:   \n","method 3 accuracy:   \n","\n","### CLIP\n","accuracy:  \n"],"metadata":{"id":"NbByJ6v6JmGa"}},{"cell_type":"markdown","metadata":{"id":"KDCaQyb7M6b1"},"source":["# Convert to PDF"]},{"cell_type":"code","source":["# Please provide the full path of the notebook file below\n","# Important: make sure that your file name does not contain spaces!\n","import os\n","notebookpath = '' # Ex: notebookpath = '/content/drive/My Drive/Colab Notebooks/EECS 442 Fall 2023 - PS7.ipynb'\n","drive_mount_point = '/content/drive/'\n","from google.colab import drive\n","drive.mount(drive_mount_point)\n","file_name = notebookpath.split('/')[-1]\n","get_ipython().system(\"apt update && apt install texlive-xetex texlive-fonts-recommended texlive-generic-recommended\")\n","get_ipython().system(\"pip install pypandoc\")\n","get_ipython().system(\"apt-get install texlive texlive-xetex texlive-latex-extra pandoc\")\n","get_ipython().system(\"jupyter nbconvert --to PDF {}\".format(notebookpath.replace(' ', '\\\\ ')))\n","from google.colab import files\n","files.download(notebookpath.split('.')[0]+'.pdf')"],"metadata":{"id":"mN7LKQubfX3b"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["ix5dQS2rUMlu","apEPzDNtK0MC"],"provenance":[{"file_id":"19mBlXSX-KqsjtayczBFfSTZ11RifjlEq","timestamp":1703323819757}],"gpuClass":"premium"},"gpuClass":"premium","kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}