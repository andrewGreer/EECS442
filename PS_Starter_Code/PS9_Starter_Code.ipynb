{"cells":[{"cell_type":"markdown","metadata":{"id":"9Z3xs9PlnzCX"},"source":["# **EECS 442 PS9: Neural Radiance Fields**\n","\n","__Please provide the following information__\n","(e.g. Andrew Owens, ahowens):\n","\n","[Your first name] [Your last name], [Your UMich uniqname]"]},{"cell_type":"markdown","source":["## Brief Overview\n","\n","In this problem set, you will implement Neural Radiance Fields (NeRF).\n","\n","A NeRF is an MLP that can generate novel views 3D scenes. It is trained on a set of 2D images with camera poses. The network takes viewing direction and spatial location (5D vector) as input and predicts opacity and RGB color (4D vector), and the novel views are rendered using volume rendering.\n","\n"],"metadata":{"id":"mqYCtrFiSePN"}},{"cell_type":"markdown","metadata":{"id":"bONWuwTInzCZ"},"source":["## 9.0 Starting"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"1SmS-7_QZydQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"67_c5zMWnzCZ"},"outputs":[],"source":["!pip install imageio-ffmpeg\n","!pip install torchsummary"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pTZB_snanzCa"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import random\n","from tqdm.notebook import tqdm\n","import os, imageio\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torchvision\n","import torchvision.transforms.functional as TF\n","from torchsummary import summary\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","if torch.cuda.is_available():\n","    print(\"Using the GPU!\")\n","else:\n","    print(\"WARNING: Could not find GPU! Using CPU only. If you want to enable GPU, please to go Edit > Notebook Settings > Hardware Accelerator and select GPU.\")"]},{"cell_type":"markdown","metadata":{"id":"xWBh21A3nzCb"},"source":["## 9.1 Fit Single Image using MLP\n","\n","In 9.1, we will first study the importance of positional encoding.\n","\n","We will fit a single image using an MLP. The MLP takes the 2D coordinate of each pixel as input and directly predict its RGB value. Our experiments will show that different positional encoding methods will significantly affect the result fidelity.\n","\n","Functions to implement:\n","\n","\n","1.   (2 points) ${\\tt input\\_mapping(self, x, B)}$ in 9.1.1\n","2.   (1 points) base Fourier features matrix ${\\tt B}$ in 9.1.4\n","3.   (1 points) full Fourier features matrix ${\\tt B}$ in 9.1.5\n","\n"]},{"cell_type":"markdown","metadata":{"id":"L_gYNfmxnzCb"},"source":["### 9.1.0 Setup Data\n","\n","Let's first download a photo from flickr and use it as our target image."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"apDxHqZonzCb"},"outputs":[],"source":["# Download image, take a square crop from the center\n","image_url = 'https://live.staticflickr.com/7492/15677707699_d9d67acf9d_b.jpg'\n","img = imageio.v2.imread(image_url)[..., :3] / 255.\n","c = [img.shape[0]//2, img.shape[1]//2]\n","r = 256\n","img = img[c[0]-r:c[0]+r, c[1]-r:c[1]+r]\n","plt.imshow(img)\n","plt.show()\n","# Create input pixel coordinates in the unit square\n","coords = np.linspace(0, 1, img.shape[0], endpoint=False)\n","input_coords = np.stack(np.meshgrid(coords, coords), -1)\n","# Define input and target\n","input_coords = TF.to_tensor(input_coords)\n","target_img = TF.to_tensor(img)"]},{"cell_type":"markdown","source":["### 9.1.1 Implement MLP\n","\n","We will first build an MLP that takes 2D coordinates as input and apply different input mapping methods.\n","\n","If the Fourier feature mapping matrix is provided, the model will first encode the input $x$ into $\\gamma(x)=(\\text{sin}(2\\pi x),\\text{sin}(2^{2}\\pi x), \\cdots, \\text{sin}(2^{n}\\pi x), \\text{cos}(2\\pi x),\\text{cos}(2^{2}\\pi x), \\cdots, \\text{cos}(2^{n}\\pi x))$.\n","\n","You will be asked to implement the ${\\tt input\\_mapping(self, x, B)}$ function.\n","\n","Here's an example of the input and output of this function:\n"," - x: [[[-1, 1]]] # one point including x and y coordinates\n"," - B: [[1, 0], [0, 1]]\n"," - input\\_mapping(x, B): [[[$\\text{sin}(-2\\pi)$, $\\text{sin}(2\\pi)$, $\\text{cos}(-2\\pi)$, $\\text{cos}(2\\pi)$]]]\n"],"metadata":{"id":"IqX09hBYJr8h"}},{"cell_type":"code","source":["class MLP(nn.Module):\n","  \"\"\"\n","  A simple Multilayer Perceptron (MLP) model designed to fit a single image. The model takes 2D coordinates as input\n","  and predicts RGB pixel values. It optionally uses Fourier feature mapping for input transformation, which can help\n","  in learning high-frequency components in the image.\n","\n","  Parameters:\n","  B (torch.Tensor, optional): A tensor for Fourier feature mapping. If provided, it transforms the input coordinates\n","                              before passing them through the MLP. Default is None.\n","  \"\"\"\n","\n","  def __init__(self, B=None):\n","    super(MLP, self).__init__()\n","    if B is None:\n","      # Standard input dimension (2 for 2D coordinates).\n","      self.layer1 = nn.Linear(2, 64)\n","    else:\n","      # Input dimension is adjusted based on the Fourier feature tensor B.\n","      input_dim = B.shape[0] * 2\n","      self.layer1 = nn.Linear(input_dim, 64)\n","\n","    self.layer2 = nn.Linear(64, 64)\n","    self.layer3 = nn.Linear(64, 3)\n","\n","    # Store the Fourier feature tensor.\n","    self.B = B\n","\n","  # Fourier feature mapping function.\n","  def input_mapping(self, x, B):\n","    \"\"\"\n","    Apply Fourier feature mapping to the input if a tensor B is provided.\n","    This mapping uses sinusoidal functions to project input coordinates.\n","\n","    Parameters:\n","    x (torch.Tensor): The input tensor (2D coordinates).\n","    B (torch.Tensor): The tensor used for Fourier feature mapping.\n","\n","    Returns:\n","    torch.Tensor: The transformed input tensor.\n","    \"\"\"\n","    if B is None:\n","      return x\n","    else:\n","      #############################################################################\n","      #                                   TODO                                    #\n","      #############################################################################\n","      pass\n","      #############################################################################\n","      #                             END OF YOUR CODE                              #\n","      #############################################################################\n","\n","\n","  def forward(self, x):\n","    \"\"\"\n","    Defines the forward pass of the MLP.\n","\n","    Parameters:\n","    x (torch.Tensor): The input tensor (2D coordinates).\n","\n","    Returns:\n","    torch.Tensor: The output tensor representing RGB pixel values.\n","    \"\"\"\n","    # Apply Fourier feature mapping to the input.\n","    x = self.input_mapping(x, self.B)\n","\n","    x = torch.relu(self.layer1(x))\n","    x = torch.relu(self.layer2(x))\n","\n","    x = torch.sigmoid(self.layer3(x))\n","\n","    return x"],"metadata":{"id":"fmMsiUR4tLRa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 9.1.2 Trainer\n","\n","Next we build a trainer class that wraps up the training loop of the image fitting task."],"metadata":{"id":"mUT3xdipJxDn"}},{"cell_type":"code","source":["class Trainer:\n","  \"\"\"\n","  Trainer class for training an MLP model on a single image fitting task.\n","  This class handles the training loop, loss computation, optimization, and visualization of the training process.\n","\n","  Parameters:\n","  model (torch.nn.Module): The MLP model.\n","  input_coords (torch.Tensor): The input coordinates for the MLP model.\n","  target_img (torch.Tensor): The target image that the MLP model aims to approximate.\n","  learning_rate (float): The learning rate for the optimizer.\n","  device (torch.device): The device (CPU or GPU) to run the training on.\n","  \"\"\"\n","  def __init__(self, model, input_coords, target_img, learning_rate, device):\n","    self.model = model\n","    self.optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","    self.criterion = nn.MSELoss()\n","    self.input_coords = torch.permute(input_coords, (1,2,0)).float()\n","    self.target_img = torch.permute(target_img, (1,2,0)).float()\n","    self.device = device\n","\n","  def visualize(self, outputs, targets):\n","    outputs = outputs.detach().cpu().numpy()\n","    targets = targets.detach().cpu().numpy()\n","\n","    # Assuming batch size is 1 for simplicity in visualization\n","    output_image = outputs\n","    target_image = targets\n","\n","    plt.figure(figsize=(6, 3))\n","\n","    plt.subplot(1, 2, 1)\n","    plt.imshow(np.clip(output_image, 0, 1))\n","    plt.title(\"Prediction\")\n","\n","    plt.subplot(1, 2, 2)\n","    plt.imshow(np.clip(target_image, 0, 1))\n","    plt.title(\"Target\")\n","\n","    plt.show()\n","\n","  def train(self, iterations=1000):\n","    self.model.train()\n","    for iter in tqdm(range(iterations),leave=False):\n","      inputs, targets = self.input_coords.to(self.device), self.target_img.to(self.device)\n","      self.optimizer.zero_grad()\n","      outputs = self.model(inputs)\n","      loss = self.criterion(outputs, targets)\n","      loss.backward()\n","      self.optimizer.step()\n","\n","      if iter % (iterations//2) == 0 or iter == iterations-1:\n","        self.visualize(outputs, targets)\n","        print(f'Iteration [{iter+1}/{iterations}], Loss: {loss.item():.4f}')"],"metadata":{"id":"KTfIQn4rsALc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 9.1.3 Train with raw coordinates\n","\n","Let's first try fitting the image using the raw 2D coordinates.\n","\n","Due to lack of high frequency information, the network prediction will be very blurry."],"metadata":{"id":"_LRFi9nLJz_5"}},{"cell_type":"code","source":["mlp = MLP(B=None).to(device)\n","trainer = Trainer(\n","    model = mlp,\n","    input_coords = input_coords,\n","    target_img = target_img,\n","    learning_rate = 1e-2,\n","    device = device\n",")\n","trainer.train(1000)"],"metadata":{"id":"mQ7PBirUuqE_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 9.1.4 Train with base Fourier features\n","\n","Next, we apply a base Fourier positional encoding.\n","\n","Given an input $x$, we will encode it into $\\gamma(x)=(\\text{sin}(2\\pi x),\\text{cos}(2\\pi x))$.\n","\n","The result will be significantly better than raw 2D coordinates, but the details will still be missing."],"metadata":{"id":"ADF82preJ6Xf"}},{"cell_type":"code","source":["feature_dim = 2\n","B = torch.zeros((feature_dim,2)).to(device)\n","#############################################################################\n","#                                   TODO                                    #\n","#############################################################################\n","pass\n","#############################################################################\n","#                             END OF YOUR CODE                              #\n","#############################################################################\n","mlp = MLP(B=B).to(device)\n","trainer = Trainer(\n","    model = mlp,\n","    input_coords = input_coords,\n","    target_img = target_img,\n","    learning_rate = 1e-2,\n","    device = device\n",")\n","trainer.train(1000)"],"metadata":{"id":"Wj6lHUEd26L8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 9.1.5 Train with full Fourier features\n","\n","Finally, we apply a full version of Fourier positional encoding.\n","\n","Given an input $x$, we will encode it into $\\gamma(x)=(\\text{sin}(2\\pi x),\\text{sin}(2^{2}\\pi x), \\cdots, \\text{sin}(2^{n}\\pi x), \\text{cos}(2\\pi x),\\text{cos}(2^{2}\\pi x), \\cdots, \\text{cos}(2^{n}\\pi x))$.\n","\n","This will force the network to pay attention to high frequencies and achieve realistic prediction that is almost the same as the target image."],"metadata":{"id":"Zr1-4S0SJ988"}},{"cell_type":"code","source":["feature_dim = 64\n","B = torch.zeros((feature_dim,2)).to(device)\n","#############################################################################\n","#                                   TODO                                    #\n","#############################################################################\n","pass\n","#############################################################################\n","#                             END OF YOUR CODE                              #\n","#############################################################################\n","mlp = MLP(B=B).to(device)\n","trainer = Trainer(\n","    model = mlp,\n","    input_coords = input_coords,\n","    target_img = target_img,\n","    learning_rate = 1e-2,\n","    device = device\n",")\n","trainer.train(1000)"],"metadata":{"id":"bIsDXISx4rlE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6md9X0oGnzCb"},"source":["## 9.2 Implementation of NeRF\n","\n","In 9.2, we will implement the NeRF from scratch and build the training pipeline.\n","\n","We will first generate rays based on camera poses, query the network with positions and directions, and finally use volume rendering to obtain the RGB output.\n","\n","Fuctions to implement:\n","\n","1.   Function ${\\tt positional\\_encoder(x, L\\_embed=6)}$ in 9.2.2\n","2.   Calculation of the 3D points sampled along the rays in 9.2.5\n","2.   Calculation of ${\\tt rgb\\_map}$, ${\\tt depth\\_map}$ in 9.2.5"]},{"cell_type":"markdown","source":["### 9.2.0 Setup data\n","\n","To minimize training time, we will use a tiny dataset the only contains 106 images and their camera poses."],"metadata":{"id":"2tTJlVKjnnap"}},{"cell_type":"code","source":["%%capture\n","!wget -O tiny_nerf_data.npz 'https://drive.google.com/uc?export=download&id=1gt6PXUo9DQgZUEhw-CZJHqr7l0uGO-db'"],"metadata":{"id":"iVDJy9JwOv8x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["rawData = np.load(\"tiny_nerf_data.npz\",allow_pickle=True)\n","images = rawData[\"images\"]\n","poses = rawData[\"poses\"]\n","focal = rawData[\"focal\"]\n","H, W = images.shape[1:3]\n","H = int(H)\n","W = int(W)\n","print(\"Images: {}\".format(images.shape))\n","print(\"Camera Poses: {}\".format(poses.shape))\n","print(\"Focal Length: {:.4f}\".format(focal))\n","\n","testimg, testpose = images[99], poses[99]\n","plt.imshow(testimg)\n","plt.title('Dataset example')\n","plt.show()\n","images = torch.Tensor(images).to(device)\n","poses = torch.Tensor(poses).to(device)\n","testimg = torch.Tensor(testimg).to(device)\n","testpose = torch.Tensor(testpose).to(device)"],"metadata":{"id":"vPkNafKsOHZL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 9.2.1 Generate Rays\n","\n","Ray generation is a core component of NeRF. We will first implement a function that generates the camera rays for each pixel.\n","\n","Each ray includes two components: 1) origin and 2) direction."],"metadata":{"id":"VUIKCIqXoyVm"}},{"cell_type":"code","source":["def get_rays(H, W, focal, pose):\n","  \"\"\"\n","  This function generates camera rays for each pixel in an image. It calculates the origin and direction of rays\n","  based on the camera's intrinsic parameters (focal length) and extrinsic parameters (pose).\n","  The rays are generated in world coordinates, which is crucial for the NeRF rendering process.\n","\n","  Parameters:\n","  H (int): Height of the image in pixels.\n","  W (int): Width of the image in pixels.\n","  focal (float): Focal length of the camera.\n","  pose (torch.Tensor): Camera pose matrix of size 4x4.\n","\n","  Returns:\n","  tuple: A tuple containing two elements:\n","      rays_o (torch.Tensor): Origins of the rays in world coordinates.\n","      rays_d (torch.Tensor): Directions of the rays in world coordinates.\n","  \"\"\"\n","  # Create a meshgrid of image coordinates (i, j) for each pixel in the image.\n","  i, j = torch.meshgrid(\n","      torch.arange(W, dtype=torch.float32),\n","      torch.arange(H, dtype=torch.float32)\n","  )\n","  i = i.t()\n","  j = j.t()\n","\n","  # Calculate the direction vectors for each ray originating from the camera center.\n","  # We assume the camera looks towards -z.\n","  # The coordinates are normalized with respect to the focal length.\n","  dirs = torch.stack(\n","      [(i - W * 0.5) / focal,\n","        -(j - H * 0.5) / focal,\n","        -torch.ones_like(i)], -1\n","      ).to(device)\n","\n","  # Transform the direction vectors (dirs) from camera coordinates to world coordinates.\n","  # This is done using the rotation part (first 3 columns) of the pose matrix.\n","  rays_d = torch.sum(dirs[..., np.newaxis, :] * pose[:3, :3], -1)\n","\n","  # The ray origins (rays_o) are set to the camera position, given by the translation part (last column) of the pose matrix.\n","  # The position is expanded to match the shape of rays_d for broadcasting.\n","  rays_o = pose[:3, -1].expand(rays_d.shape)\n","\n","  # Return the origins and directions of the rays.\n","  return rays_o, rays_d"],"metadata":{"id":"SfVEBqg7PpGn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 9.2.2 Positional Encoding\n","\n","Similar to 9.1, in the NeRF we also leverage Fourier positional encoding to model the high frequencies.\n","\n","Given an input $x$, we will encode it into $\\gamma(x)=(x,\\text{sin}(2^{0} x),\\text{sin}(2^{1} x), \\cdots, \\text{sin}(2^{L\\_embed-1} x), \\text{cos}(2^{0} x),\\text{cos}(2^{1} x), \\cdots, \\text{cos}(2^{L\\_embed-1} x))$."],"metadata":{"id":"4Nwstzdaq0_K"}},{"cell_type":"code","source":["def positional_encoder(x, L_embed=6):\n","  \"\"\"\n","  This function applies positional encoding to the input tensor. Positional encoding is used in NeRF\n","  to allow the model to learn high-frequency details more effectively. It applies sinusoidal functions\n","  at different frequencies to the input.\n","\n","  Parameters:\n","  x (torch.Tensor): The input tensor to be positionally encoded.\n","  L_embed (int): The number of frequency levels to use in the encoding. Defaults to 6.\n","\n","  Returns:\n","  torch.Tensor: The positionally encoded tensor.\n","  \"\"\"\n","\n","  # Initialize a list with the input tensor.\n","  rets = [x]\n","\n","  # Loop over the number of frequency levels.\n","  for i in range(L_embed):\n","    #############################################################################\n","    #                                   TODO                                    #\n","    #############################################################################\n","    pass\n","    #############################################################################\n","    #                             END OF YOUR CODE                              #\n","    #############################################################################\n","\n","\n","  # Concatenate the original and encoded features along the last dimension.\n","  return torch.cat(rets, -1)"],"metadata":{"id":"x1E7EAtdpdda"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 9.2.3 Cumulative Product\n","\n","The RGB value of each pixel is computed by a weighted sum of the points along the ray. Higher weight indicates higher opacity, which means the further points will be occluded. The cumulative product fuction ensures this rendering procedure."],"metadata":{"id":"15lLqmVRsJ1w"}},{"cell_type":"code","source":["def cumprod_exclusive(tensor: torch.Tensor):\n","  \"\"\"\n","  Compute the exclusive cumulative product of a tensor along its last dimension.\n","  'Exclusive' means that the cumulative product at each element does not include the element itself.\n","  This function is used in volume rendering to compute the product of probabilities\n","  along a ray, excluding the current sample point.\n","\n","  Parameters:\n","  tensor (torch.Tensor): The input tensor for which to calculate the exclusive cumulative product.\n","\n","  Returns:\n","  torch.Tensor: The tensor after applying the exclusive cumulative product.\n","  \"\"\"\n","\n","  # Compute the cumulative product along the last dimension of the tensor.\n","  cumprod = torch.cumprod(tensor, -1)\n","\n","  # Roll the elements along the last dimension by one position.\n","  # This shifts the cumulative products to make them exclusive.\n","  cumprod = torch.roll(cumprod, 1, -1)\n","\n","  # Set the first element of the last dimension to 1, as the exclusive product of the first element is always 1.\n","  cumprod[..., 0] = 1.\n","\n","  return cumprod"],"metadata":{"id":"9Zjm6C0Yppwm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 9.2.4 NeRF Model\n","\n","The main model of NeRF will be implemented here. To reduce training time, we will only use a very small MLP model as our network. The MLP takes the output of positional encoder as input and predicts the opacity and RGB value."],"metadata":{"id":"VUP1WrWct0YV"}},{"cell_type":"code","source":["class VeryTinyNerfModel(torch.nn.Module):\n","  \"\"\"\n","  A very small implementation of a Neural Radiance Field (NeRF) model. This model is a simplified\n","  version of the standard NeRF architecture, it consists of a simple feedforward neural network with three linear layers.\n","\n","  Parameters:\n","  filter_size (int): The number of neurons in the hidden layers. Default is 128.\n","  num_encoding_functions (int): The number of sinusoidal encoding functions. Default is 6.\n","  \"\"\"\n","\n","  def __init__(self, filter_size=128, num_encoding_functions=6):\n","    super(VeryTinyNerfModel, self).__init__()\n","    self.layer1 = torch.nn.Linear(3 + 3 * 2 * num_encoding_functions, filter_size)\n","    self.layer2 = torch.nn.Linear(filter_size, filter_size)\n","    self.layer3 = torch.nn.Linear(filter_size, 4)\n","    self.relu = torch.nn.functional.relu\n","\n","  def forward(self, x):\n","    x = self.relu(self.layer1(x))\n","    x = self.relu(self.layer2(x))\n","    x = self.layer3(x)\n","    return x"],"metadata":{"id":"PYMcW4ESQLdb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 9.2.5 Volume Rendering\n","\n","After the implementation of ray generation and MLP model, we can now implement the volume rendering, which is the major procedure of NeRF."],"metadata":{"id":"TQWdpm_bzR33"}},{"cell_type":"code","source":["def render(model, rays_o, rays_d, near, far, n_samples, rand=False):\n","  \"\"\"\n","  Render a scene using a Neural Radiance Field (NeRF) model. This function samples points along rays,\n","  evaluates the NeRF model at these points, and applies volume rendering techniques to produce an image.\n","\n","  Parameters:\n","  model (torch.nn.Module): The NeRF model to be used for rendering.\n","  rays_o (torch.Tensor): Origins of the rays.\n","  rays_d (torch.Tensor): Directions of the rays.\n","  near (float): Near bound for depth sampling along the rays.\n","  far (float): Far bound for depth sampling along the rays.\n","  n_samples (int): Number of samples to take along each ray.\n","  rand (bool): If True, randomize sample depths. Default is False.\n","\n","  Returns:\n","  tuple: A tuple containing the RGB map and depth map of the rendered scene.\n","  \"\"\"\n","\n","  # Sample points along each ray, from 'near' to 'far'.\n","  z = torch.linspace(near, far, n_samples).to(device)\n","  if rand:\n","    mids = 0.5 * (z[..., 1:] + z[..., :-1])\n","    upper = torch.cat([mids, z[..., -1:]], -1)\n","    lower = torch.cat([z[..., :1], mids], -1)\n","    t_rand = torch.rand(z.shape).to(device)\n","    z = lower + (upper - lower) * t_rand\n","\n","  #############################################################################\n","  #                                   TODO                                    #\n","  #############################################################################\n","  # Compute 3D coordinates of the sampled points along the rays.\n","  points = None\n","  #############################################################################\n","  #                             END OF YOUR CODE                              #\n","  #############################################################################\n","\n","  # Flatten the points and apply positional encoding.\n","  flat_points = torch.reshape(points, [-1, points.shape[-1]])\n","  flat_points = positional_encoder(flat_points)\n","\n","  # Evaluate the model on the encoded points in chunks to manage memory usage.\n","  chunk = 1024 * 32\n","  raw = torch.cat([model(flat_points[i:i + chunk]) for i in range(0, flat_points.shape[0], chunk)], 0)\n","  raw = torch.reshape(raw, list(points.shape[:-1]) + [4])\n","\n","  # Compute densities (sigmas) and RGB values from the model's output.\n","  sigma = F.relu(raw[..., 3])\n","  rgb = torch.sigmoid(raw[..., :3])\n","\n","  # Perform volume rendering to obtain the weights of each point.\n","  one_e_10 = torch.tensor([1e10], dtype=rays_o.dtype).to(device)\n","  dists = torch.cat((z[..., 1:] - z[..., :-1], one_e_10.expand(z[..., :1].shape)), dim=-1)\n","  alpha = 1. - torch.exp(-sigma * dists)\n","  weights = alpha * cumprod_exclusive(1. - alpha + 1e-10)\n","\n","  #############################################################################\n","  #                                   TODO                                    #\n","  #############################################################################\n","  # Compute the weighted sum of RGB values along each ray to get the final pixel color.\n","  rgb_map = None\n","\n","  # Compute the depth map as the weighted sum of sampled depths.\n","  depth_map = None\n","  #############################################################################\n","  #                             END OF YOUR CODE                              #\n","  #############################################################################\n","\n","  return rgb_map, depth_map"],"metadata":{"id":"X1Lbjn0EqZba"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 9.2.6 Training Loop\n","\n","In the training loop, the model is trained to fit one image randomly picked from the dataset at each iteration."],"metadata":{"id":"8RRNA8b5z0ql"}},{"cell_type":"code","source":["mse2psnr = lambda x : -10. * torch.log(x) / torch.log(torch.Tensor([10.])).to(device)\n","\n","def train(model, optimizer, n_iters=3000):\n","  \"\"\"\n","  Train the Neural Radiance Field (NeRF) model. This function performs training over a specified number of iterations,\n","  updating the model parameters to minimize the difference between rendered and actual images.\n","\n","  Parameters:\n","  model (torch.nn.Module): The NeRF model to be trained.\n","  optimizer (torch.optim.Optimizer): The optimizer used for training the model.\n","  n_iters (int): The number of iterations to train the model. Default is 3000.\n","  \"\"\"\n","\n","  psnrs = []\n","  iternums = []\n","\n","  plot_step = 500\n","  n_samples = 64   # Number of samples along each ray.\n","\n","  for i in tqdm(range(n_iters)):\n","    # Randomly select an image from the dataset and use it as the target for training.\n","    images_idx = np.random.randint(images.shape[0])\n","    target = images[images_idx]\n","    pose = poses[images_idx]\n","\n","    # Compute the rays for the selected image.\n","    rays_o, rays_d = get_rays(H, W, focal, pose)\n","    # Render the scene using the current model state.\n","    rgb, depth = render(model, rays_o, rays_d, near=2., far=6., n_samples=n_samples, rand=True)\n","\n","    optimizer.zero_grad()\n","    image_loss = torch.nn.functional.mse_loss(rgb, target)\n","    image_loss.backward()\n","    optimizer.step()\n","\n","    if i % plot_step == 0:\n","      torch.save(model.state_dict(), 'ckpt.pth')\n","      # Render a test image to evaluate the current model performance.\n","      with torch.no_grad():\n","        rays_o, rays_d = get_rays(H, W, focal, testpose)\n","        rgb, depth = render(model, rays_o, rays_d, near=2., far=6., n_samples=n_samples)\n","        loss = torch.nn.functional.mse_loss(rgb, testimg)\n","        # Calculate PSNR for the rendered image.\n","        psnr = mse2psnr(loss)\n","\n","        psnrs.append(psnr.detach().cpu().numpy())\n","        iternums.append(i)\n","\n","        # Plotting the rendered image and PSNR over iterations.\n","        plt.figure(figsize=(9, 3))\n","\n","        plt.subplot(131)\n","        picture = rgb.cpu()  # Copy the rendered image from GPU to CPU.\n","        plt.imshow(picture)\n","        plt.title(f'RGB Iter {i}')\n","\n","        plt.subplot(132)\n","        picture = depth.cpu() * (rgb.cpu().mean(-1)>1e-2)\n","        plt.imshow(picture, cmap='gray')\n","        plt.title(f'Depth Iter {i}')\n","\n","        plt.subplot(133)\n","        plt.plot(iternums, psnrs)\n","        plt.title('PSNR')\n","        plt.show()\n"],"metadata":{"id":"YdQs6VGTQIqR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["nerf = VeryTinyNerfModel()\n","nerf = nn.DataParallel(nerf).to(device)\n","optimizer = torch.optim.Adam(nerf.parameters(), lr=5e-3, eps = 1e-7)\n","train(nerf, optimizer)"],"metadata":{"id":"33kxNTrIQNr1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 9.2.7 Rendering from different view points.\n","\n","Given the trained model, we can query it with different camera poses.\n","In this example, we simply pick three camera poses from the dataset and render the prediction images from them."],"metadata":{"id":"vB49SEEF1AjA"}},{"cell_type":"code","source":["nerf = VeryTinyNerfModel()\n","nerf = nn.DataParallel(nerf).to(device)\n","ckpt = torch.load('ckpt.pth')\n","nerf.load_state_dict(ckpt)\n","test_img_idx_list = [0, 40, 80]\n","H, W = 100, 100\n","with torch.no_grad():\n","  for test_img_idx in test_img_idx_list:\n","    rays_o, rays_d = get_rays(H, W, focal, poses[test_img_idx])\n","    rgb, depth = render(nerf, rays_o, rays_d, near=2., far=6., n_samples=64)\n","    plt.figure(figsize=(9,3))\n","\n","    plt.subplot(131)\n","    picture = rgb.cpu()\n","    plt.title(\"RGB Prediction #{}\".format(test_img_idx))\n","    plt.imshow(picture)\n","\n","    plt.subplot(132)\n","    picture = depth.cpu() * (rgb.cpu().mean(-1)>1e-2)\n","    plt.imshow(picture, cmap='gray')\n","    plt.title(\"Depth Prediction #{}\".format(test_img_idx))\n","\n","    plt.subplot(133)\n","    plt.title(\"Ground Truth #{}\".format(test_img_idx))\n","    plt.imshow(rawData[\"images\"][test_img_idx])\n","    plt.show()"],"metadata":{"id":"TamsArO0b_1G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# generate pdf\n","# %%capture\n","get_ipython().system(\"pip install pypandoc\")\n","get_ipython().system(\"apt-get install texlive texlive-xetex texlive-latex-extra pandoc\")\n","drive_mount_point = \"/content/drive/\"\n","gdrive_home = os.path.join(drive_mount_point, \"My Drive/\")\n","notebookpath=\"/content/drive/MyDrive/Colab Notebooks/\"\n","# change the name to your ipynb file name shown on the top left of Colab window\n","# Important: make sure that your file name does not contain spaces!\n","file_name=\"Starter_EECS442_FA23_PS9_Neural_Radiance_Fields.ipynb\"\n","get_ipython().system(\n","  f\"jupyter nbconvert --output-dir='{gdrive_home}' '{notebookpath}''{file_name}' --to pdf\"\n",")"],"metadata":{"id":"ABmZjsbPHKcn"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.16"},"colab":{"provenance":[{"file_id":"1VnwGKCKjX-Jo_8NFrl3asPrMNWBo1ZGG","timestamp":1703323923348}],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}