{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1FRzeBgbh89DURjgwOVr2KlWZqL4qfEFx","timestamp":1703323673774}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"cells":[{"cell_type":"markdown","metadata":{"id":"ix5dQS2rUMlu"},"source":["#EECS 442 PS5: Scene Recognition\n","\n","__Please provide the following information__\n","(e.g. Andrew Owens, ahowens):\n","\n","[Your first name] [Your last name], [Your UMich uniqname]\n","\n","__Important__: after you download the .ipynb file, please name it as __\\<your_uniqname\\>_\\<your_umid\\>.ipynb__ before you submit it to canvas. Example: adam_01101100.ipynb.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"W_Cst4k4tuBc"},"source":["# Starting\n","\n","Run the following code to import the modules you'll need. After your finish the assignment, remember to run all cells and save the note book to your local machine as a .ipynb file for Canvas submission."]},{"cell_type":"code","source":["import pickle\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import os\n","import copy\n","from tqdm import tqdm\n","\n","import torch\n","import torchvision\n","from torchvision import datasets, models, transforms\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","print(\"PyTorch Version: \",torch.__version__)\n","print(\"Torchvision Version: \",torchvision.__version__)\n","# Detect if we have a GPU available\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","if torch.cuda.is_available():\n","    print(\"Using the GPU!\")\n","else:\n","    print(\"WARNING: Could not find GPU! Using CPU only. If you want to enable GPU, please to go Edit > Notebook Settings > Hardware Accelerator and select GPU.\")\n","\n","data_dir = \"./data_miniplaces_modified\""],"metadata":{"id":"WO_-S04OPQej","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1696877883229,"user_tz":240,"elapsed":6232,"user":{"displayName":"Yiming Dou","userId":"00198832258301430987"}},"outputId":"e28e69cc-5766-4a22-a865-6178ddf8dff7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["PyTorch Version:  2.0.1+cu118\n","Torchvision Version:  0.15.2+cu118\n","Using the GPU!\n"]}]},{"cell_type":"markdown","metadata":{"id":"apEPzDNtK0MC"},"source":["# Problem 5.1 Scene Recognition with VGG\n","\n","You will build and train a convolutional neural network for scene recognition, i.e., classifying images into different scenes. You will need to:\n","1. Construct dataloaders for train/val/test datasets\n","2. Build MiniVGG and MiniVGG-BN (MiniVGG with batch-normalization layers)\n","3. Train MiniVGG and MiniVGG-BN, compare their training progresses and their final top-1 and top-5 accuracies.\n","4. (Optional) Increase the size of the network by adding more layers and check whether top-1 and top-5 accuracies will improve."]},{"cell_type":"markdown","metadata":{"id":"BY1R8P_ZD4BP"},"source":["## Step 0: Downloading the dataset.\n","\n"]},{"cell_type":"code","source":["# Download the miniplaces dataset\n","# Note: Restarting the runtime won't remove the downloaded dataset. You only need to re-download the zip file if you lose connection to colab.\n","!wget http://www.eecs.umich.edu/courses/eecs504/data_miniplaces_modified.zip"],"metadata":{"id":"ArnaoQjgPXae","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1696877951746,"user_tz":240,"elapsed":11913,"user":{"displayName":"Yiming Dou","userId":"00198832258301430987"}},"outputId":"9a31e900-3ac1-42af-8282-a3b0d41a9971"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2023-10-09 18:58:59--  http://www.eecs.umich.edu/courses/eecs504/data_miniplaces_modified.zip\n","Resolving www.eecs.umich.edu (www.eecs.umich.edu)... 141.212.113.116\n","Connecting to www.eecs.umich.edu (www.eecs.umich.edu)|141.212.113.116|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 534628730 (510M) [application/zip]\n","Saving to: ‘data_miniplaces_modified.zip’\n","\n","data_miniplaces_mod 100%[===================>] 509.86M  46.5MB/s    in 11s     \n","\n","2023-10-09 18:59:11 (44.5 MB/s) - ‘data_miniplaces_modified.zip’ saved [534628730/534628730]\n","\n"]}]},{"cell_type":"code","metadata":{"id":"0OzTJBsQU_Pp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1696878017280,"user_tz":240,"elapsed":5319,"user":{"displayName":"Yiming Dou","userId":"00198832258301430987"}},"outputId":"b3a85440-5aaf-4226-e0e5-e51053f72f69"},"source":["# Unzip the download dataset .zip file to your local colab dir\n","# Warning: this upzipping process may take a while. Please be patient.\n","!unzip -q data_miniplaces_modified.zip"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["replace data_miniplaces_modified/test/badlands/00000399.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: "]}]},{"cell_type":"markdown","metadata":{"id":"PZ8YxDO2IKHH"},"source":["## 5.1 (a): Build dataloaders for train, val, and test"]},{"cell_type":"code","metadata":{"id":"vELvzrzTpk3U"},"source":["def get_dataloaders(input_size, batch_size, shuffle = True):\n","    \"\"\"\n","    Build dataloaders with transformations.\n","\n","    Args:\n","        input_size: int, the size of the tranformed images\n","        batch_size: int, minibatch size for dataloading\n","\n","    Returns:\n","        dataloader_dict: dict, dict with \"train\", \"val\", \"test\" keys, each is mapped to a pytorch dataloader.\n","    \"\"\"\n","\n","    mean = [0.485, 0.456, 0.406]\n","    std = [0.229, 0.224, 0.225]\n","\n","    ###########################################################################\n","    # TODO: Step 1: Build transformations for the dataset.                    #\n","    # You need to construct a data transformation that does three             #\n","    # preprocessing steps in order:                                           #\n","    # I. Resize the image to input_size using transforms.Resize               #\n","    # II. Convert the image to PyTorch tensor using transforms.ToTensor       #\n","    # III. Normalize the images with the provided mean and std parameters     #\n","    # using transforms.Normalize. These parameters are accumulated from a     #\n","    # large number of training samples.                                       #\n","    # You can use transforms.Compose to combine the above three               #\n","    # transformations. Store the combined transforms in the variable          #\n","    # 'composed_transform'.                                                   #\n","    ###########################################################################\n","\n","\n","    ###########################################################################\n","    #                              END OF YOUR CODE                           #\n","    ###########################################################################\n","\n","    # We write the remaining part of the dataloader for you.\n","    # You are encouraged to go through this.\n","\n","    ###########################################################################\n","    # Step 2: Build dataloaders.                                              #\n","    # I. We use torch.datasets.ImageFolder with the provided data_dir and the #\n","    # data transfomations you created in step 1 to construct pytorch datasets #\n","    # for training, validation, and testing.                                  #\n","    # II. Then we use torch.utils.data.DataLoader to build dataloaders with   #\n","    # the constructed pytorch datasets. You need to enable shuffling for      #\n","    # the training set. Set num_workers=2 to speed up dataloading.            #\n","    # III. Finally, we put the dataloaders into a dictionary.                 #\n","    ###########################################################################\n","\n","    # Create train/val/test datasets\n","    data_transforms = {\n","        'train': composed_transform,\n","        'val': composed_transform,\n","        'test': composed_transform\n","    }\n","    image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in data_transforms.keys()}\n","\n","    # Create training train/val/test dataloaders\n","    # Never shuffle the val and test datasets\n","    dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=False if x != 'train' else shuffle, num_workers=2) for x in data_transforms.keys()}\n","\n","    return dataloaders_dict"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["batch_size = 16\n","input_size = 128\n","dataloaders_dict = get_dataloaders(input_size, batch_size)\n","\n","# Confirm your train/val/test sets contain 90,000/10,000/10,000 samples\n","print('# of training samples {}'.format(len(dataloaders_dict['train'].dataset)))\n","print('# of validation samples {}'.format(len(dataloaders_dict['val'].dataset)))\n","print('# of test samples {}'.format(len(dataloaders_dict['test'].dataset)))"],"metadata":{"id":"L-D8H_Z3TBG8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Visualize the data within the dataset\n","import json\n","with open('./data_miniplaces_modified/category_names.json', 'r') as f:\n","    class_names = json.load(f)['i2c']\n","class_names = {i:name for i, name in enumerate(class_names)}\n","\n","def imshow(inp, title=None, ax=None, figsize=(10, 10)):\n","  \"\"\"Imshow for Tensor.\"\"\"\n","  inp = inp.numpy().transpose((1, 2, 0))\n","  mean = np.array([0.485, 0.456, 0.406])\n","  std = np.array([0.229, 0.224, 0.225])\n","  inp = std * inp + mean\n","  inp = np.clip(inp, 0, 1)\n","  if ax is None:\n","    fig, ax = plt.subplots(1, figsize=figsize)\n","  ax.imshow(inp)\n","  ax.set_xticks([])\n","  ax.set_yticks([])\n","  if title is not None:\n","    ax.set_title(title)\n","\n","# Get a batch of training data\n","inputs, classes = next(iter(dataloaders_dict['train']))\n","\n","# Make a grid from batch\n","out = torchvision.utils.make_grid(inputs, nrow=4)\n","\n","fig, ax = plt.subplots(1, figsize=(10, 10))\n","title = [class_names[x.item()] if (i+1) % 4 != 0 else class_names[x.item()]+'\\n' for i, x in enumerate(classes)]\n","imshow(out, title=' | '.join(title), ax=ax)"],"metadata":{"id":"UmBIoXYgTCex"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wMV6Wyo-xAvR"},"source":["## 5.1 (b): Build MiniVGG and MiniVGG-BN\n","\n","Please follow the instructions to build the two neural networks with architectures shown below.\n","\n","__MiniVGG architecture__\n","\n","![alt text](https://drive.google.com/uc?id=1RF3CjEsHBpRjubqwgBUxIEv54gpDjt95)\n","\n","__MiniVGG-BN architecure__\n","\n","![alt text](https://drive.google.com/uc?id=1HR5N2V-5RVT1u_bVzyCvhPJZ7PSnYm6x)\n"]},{"cell_type":"code","metadata":{"id":"V6yXydgmnKjg"},"source":["# Helper function for counting number of trainable parameters.\n","def count_params(model):\n","    \"\"\"\n","    Counts the number of trainable parameters in PyTorch.\n","\n","    Args:\n","        model: PyTorch model.\n","\n","    Returns:\n","        num_params: int, number of trainable parameters.\n","    \"\"\"\n","\n","    num_params = sum([item.numel() for item in model.parameters() if item.requires_grad])\n","\n","    return num_params"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dVzbyGMQxW8c"},"source":["# Network configurations for all layers before the final fully-connected layers.\n","# \"M\" corresponds to maxpooling layer, integers correspond to number of output\n","# channels of a convolutional layer.\n","cfgs = {\n","    'MiniVGG': [64, 'M', 128, 'M', 128, 128, 'M'],\n","    'MiniVGG-BN': [64, 'M', 128, 'M', 128, 128, 'M']\n","}\n","\n","def make_layers(cfg, batch_norm=False):\n","    \"\"\"\n","    Return a nn.Sequential object containing all layers to get the features\n","    using the CNN. (That is, before the Average pooling layer in the two\n","    pictures above).\n","\n","    Args:\n","      cfg: list\n","      batch_norm: bool, default: False. If set to True, a BatchNorm layer\n","                  should be added after each convolutional layer.\n","\n","    Return:\n","      features: torch.nn.Sequential. Containers for all feature extraction\n","                layers. For use of torch.nn.Sequential, please refer to\n","                PyTorch documentation.\n","    \"\"\"\n","    ###########################################################################\n","    # TODO: Construct the neural net architecture from cfg. You should use    #\n","    # nn.Sequential().                                                        #\n","    ###########################################################################\n","\n","\n","\n","    ###########################################################################\n","    #                              END OF YOUR CODE                           #\n","    ###########################################################################\n","\n","    return features\n","\n","class VGG(nn.Module):\n","\n","    def __init__(self, features, num_classes=100, init_weights=True):\n","        super(VGG, self).__init__()\n","\n","        self.features = features\n","        self.avgpool = nn.AdaptiveAvgPool2d((5, 5))\n","\n","        #######################################################################\n","        # TODO: Construct the final FC layers using nn.Sequential.            #\n","        # Note: The average pooling layer has been defined by us above.       #\n","        #######################################################################\n","\n","\n","\n","        #######################################################################\n","        #                              END OF YOUR CODE                       #\n","        #######################################################################\n","\n","        if init_weights:\n","            self._initialize_weights()\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","        x = self.avgpool(x)\n","        x = torch.flatten(x, 1)\n","        x = self.classifier(x)\n","        return x\n","\n","    def _initialize_weights(self):\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n","                if m.bias is not None:\n","                    nn.init.constant_(m.bias, 0)\n","            elif isinstance(m, nn.BatchNorm2d):\n","                nn.init.constant_(m.weight, 1)\n","                nn.init.constant_(m.bias, 0)\n","            elif isinstance(m, nn.Linear):\n","                nn.init.normal_(m.weight, 0, 0.01)\n","                nn.init.constant_(m.bias, 0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["features = make_layers(cfgs['MiniVGG'], batch_norm=False)\n","vgg = VGG(features)\n","\n","features = make_layers(cfgs['MiniVGG-BN'], batch_norm=True)\n","vgg_bn = VGG(features)\n","\n","# Print the network architectrue. Please compare the printed architecture with\n","# the one given in the instructions above.\n","# Make sure your network has the same architecture as the one we give above.\n","print(vgg)\n","print('Number of trainable parameters {}'.format(count_params(vgg)))\n","\n","print(vgg_bn)\n","print('Number of trainable parameters {}'.format(count_params(vgg_bn)))"],"metadata":{"id":"gNa1PC15TGr9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Nu5lRDqRt-A8"},"source":["## 5.1 (c): Build training/validation loops\n","\n","You will write a function for training and validating the network."]},{"cell_type":"code","metadata":{"id":"9A1x0QJlujAB"},"source":["def make_optimizer(model):\n","    \"\"\"\n","    Args:\n","        model: NN to train\n","\n","    Returns:\n","        optimizer: pytorch optmizer for updating the given model parameters.\n","    \"\"\"\n","    ###########################################################################\n","    # TODO: Create a SGD optimizer with learning rate=1e-2 and momentum=0.9.  #\n","    # HINT: Check out optim.SGD() and initialize it with the appropriate      #\n","    # parameters. We have imported torch.optim as optim for you above.        #\n","    ###########################################################################\n","\n","    ###########################################################################\n","    #                              END OF YOUR CODE                           #\n","    ###########################################################################\n","    return optimizer\n","\n","def get_loss():\n","    \"\"\"\n","    Returns:\n","        criterion: pytorch loss.\n","    \"\"\"\n","    ###########################################################################\n","    # TODO: Create an instance of the cross entropy loss. This code           #\n","    # should be a one-liner.                                                  #\n","    ###########################################################################\n","\n","    ###########################################################################\n","    #                              END OF YOUR CODE                           #\n","    ###########################################################################\n","\n","    return criterion"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AYGmDsrzY4Ir"},"source":["def train_model(model, dataloaders, criterion, optimizer, save_dir = None, num_epochs=25, model_name='MiniVGG'):\n","    \"\"\"\n","    Args:\n","        model: The NN to train\n","        dataloaders: A dictionary containing at least the keys\n","                    'train','val' that maps to Pytorch data loaders for the dataset\n","        criterion: The Loss function\n","        optimizer: Pytroch optimizer. The algorithm to update weights\n","        num_epochs: How many epochs to train for\n","        save_dir: Where to save the best model weights that are found. Using None will not write anything to disk.\n","\n","    Returns:\n","        model: The trained NN\n","        tr_acc_history: list, training accuracy history. Recording freq: one epoch.\n","        val_acc_history: list, validation accuracy history. Recording freq: one epoch.\n","    \"\"\"\n","\n","    val_acc_history = []\n","    tr_acc_history = []\n","\n","    best_model_wts = copy.deepcopy(model.state_dict())\n","    best_acc = 0.0\n","\n","    for epoch in range(num_epochs):\n","        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n","        print('-' * 10)\n","\n","        # Each epoch has a training and validation phase\n","        for phase in ['train', 'val']:\n","            if phase == 'train':\n","                model.train()  # Set model to training mode\n","            else:\n","                model.eval()   # Set model to evaluate mode\n","\n","            # loss and number of correct prediction for the current batch\n","            running_loss = 0.0\n","            running_corrects = 0\n","\n","            # Iterate over data.\n","            # TQDM has nice progress bars\n","            for inputs, labels in tqdm(dataloaders[phase]):\n","                inputs = inputs.to(device)\n","                labels = labels.to(device)\n","\n","                ###############################################################\n","                # TODO:                                                       #\n","                # Please read all the inputs carefully!                       #\n","                # For \"train\" phase:                                          #\n","                # (i)   Compute the outputs using the model                   #\n","                #       Also, use the  outputs to calculate the class         #\n","                #       predicted by the model,                               #\n","                #       Store the predicted class in 'preds'                  #\n","                #       (Think: argmax of outputs across a dimension)         #\n","                #       torch.max() might help!                               #\n","                # (ii)  Use criterion to store the loss in 'loss'             #\n","                # (iii) Update the model parameters                           #\n","                # Notes:                                                      #\n","                # - Don't forget to zero the gradients before beginning the   #\n","                # loop!                                                       #\n","                # - \"val\" phase is the same as train, but without backprop    #\n","                # - Compute the outputs (Same as \"train\", calculate 'preds'   #\n","                # too),                                                       #\n","                # - Calculate the loss and store it in 'loss'                 #\n","                ###############################################################\n","\n","\n","\n","\n","                ###############################################################\n","                #                         END OF YOUR CODE                    #\n","                ###############################################################\n","\n","                # statistics\n","                running_loss += loss.item() * inputs.size(0)\n","                running_corrects += torch.sum(preds == labels.data)\n","\n","            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n","            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n","\n","            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n","\n","            # deep copy the model\n","            if phase == 'val' and epoch_acc > best_acc:\n","                best_acc = epoch_acc\n","                best_model_wts = copy.deepcopy(model.state_dict())\n","\n","                # save the best model weights\n","                # =========================================================== #\n","                # IMPORTANT:\n","                # Losing your connection to colab will lead to loss of trained\n","                # weights.\n","                # You should download the trained weights to your local machine.\n","                # Later, you can load these weights directly without needing to\n","                # train the neural networks again.\n","                # =========================================================== #\n","                if save_dir:\n","                    torch.save(best_model_wts, os.path.join(save_dir, model_name + '.pth'))\n","\n","            # record the train/val accuracies\n","            if phase == 'val':\n","                val_acc_history.append(epoch_acc)\n","            else:\n","                tr_acc_history.append(epoch_acc)\n","\n","    print('Best val Acc: {:4f}'.format(best_acc))\n","\n","    return model, tr_acc_history, val_acc_history"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ay6JSulvBcSS"},"source":["## 5.1 (d): Train MiniVGG / MiniVGG-BN"]},{"cell_type":"code","metadata":{"id":"fqEGeODmZJtM"},"source":["# Number of classes in the dataset\n","# Miniplaces has 100\n","num_classes = 100\n","\n","# Batch size for training\n","batch_size = 128\n","\n","# Shuffle the input data?\n","shuffle_datasets = True\n","\n","# Number of epochs to train for\n","# During debugging, you can set this parameter to 1\n","# num_epochs = 1\n","# Training for 20 epochs. This will take about half an hour.\n","num_epochs = 20\n","\n","### IO\n","# Directory to save weights to\n","save_dir = \"weights\"\n","os.makedirs(save_dir, exist_ok=True)\n","\n","# get dataloaders and criterion function\n","input_size = 64\n","dataloaders = get_dataloaders(input_size, batch_size, shuffle_datasets)\n","criterion = get_loss()"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Initialize MiniVGG\n","features = make_layers(cfgs['MiniVGG'], batch_norm=False)\n","model = VGG(features).to(device)\n","optimizer = make_optimizer(model)\n","\n","# Train the model!\n","vgg, tr_his, val_his = train_model(model=model, dataloaders=dataloaders, criterion=criterion, optimizer=optimizer,\n","           save_dir=save_dir, num_epochs=num_epochs, model_name='MiniVGG')"],"metadata":{"id":"rK8qQ240TK3R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Initialize MiniVGG-BN\n","features = make_layers(cfgs['MiniVGG-BN'], batch_norm=True)\n","model = VGG(features).to(device)\n","optimizer = make_optimizer(model)\n","\n","# Train the model!\n","vgg_BN, tr_his_BN, val_his_BN = train_model(model=model, dataloaders=dataloaders, criterion=criterion, optimizer=optimizer,\n","           save_dir=save_dir, num_epochs=num_epochs, model_name='MiniVGG-BN')"],"metadata":{"id":"vfk0-O25TMJv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x = np.arange(num_epochs)\n","# train/val accuracies for MiniVGG\n","plt.figure()\n","plt.plot(x, torch.tensor(tr_his, device = 'cpu'))\n","plt.plot(x, torch.tensor(val_his, device = 'cpu'))\n","plt.legend(['Training top1 accuracy', 'Validation top1 accuracy'])\n","plt.xticks(x)\n","plt.xlabel('Epoch')\n","plt.ylabel('Top1 Accuracy')\n","plt.title('MiniVGG')\n","plt.show()\n","\n","# train/val accuracies for MiniVGG-BN\n","plt.plot(x, torch.tensor(tr_his_BN, device = 'cpu'))\n","plt.plot(x, torch.tensor(val_his_BN, device = 'cpu'))\n","plt.legend(['Training top1 accuracy', 'Validation top1 accuracy'])\n","plt.xticks(x)\n","plt.xlabel('Epoch')\n","plt.ylabel('Top1 Accuracy')\n","plt.title('MiniVGG-BN')\n","plt.show()\n","\n","# compare val accuracies of MiniVGG and MiniVGG-BN\n","plt.plot(x, torch.tensor(val_his, device = 'cpu'))\n","plt.plot(x, torch.tensor(val_his_BN, device = 'cpu'))\n","plt.legend(['MiniVGG', 'MiniVGG-BN'])\n","plt.xticks(x)\n","plt.xlabel('Epoch')\n","plt.ylabel('Top1 Accuracy')\n","plt.title('Compare')\n","plt.show()"],"metadata":{"id":"yJKf7xsTTc79"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sWouQAds7fif"},"source":["### TODO: __Summarize the effect of batch normalization:__\n","\n","\n","\n","\n"]},{"cell_type":"code","source":["# Answer here"],"metadata":{"id":"qXGPe80MTjkZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VMgbQ0FwfqWH"},"source":["pickle.dump(tr_his, open('tr_his.pkl', 'wb'))\n","pickle.dump(tr_his_BN, open('tr_his_BN.pkl', 'wb'))\n","pickle.dump(val_his, open('val_his.pkl', 'wb'))\n","pickle.dump(val_his_BN, open('val_his_BN.pkl', 'wb'))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G6usP91uywfF"},"source":["## Step 6. Measure top1 and top5 accuracies of MiniVGG and MiniVGG-BN\n","\n","__Definition of top-k accuracy__: if the correct label is within the _top k_ predicted classes according to the network output scores, we count the prediction by the neural network as a correct prediction."]},{"cell_type":"code","metadata":{"id":"5ZdFjScPblUg"},"source":["def accuracy(output, target, topk=(1,)):\n","    \"\"\"\n","    Computes the accuracy over the k top predictions for the specified values\n","    of k.\n","\n","    Args:\n","        output: pytorch tensor, (batch_size x num_classes). Outputs of the\n","                network for one batch.\n","        target: pytorch tensor, (batch_size,). True labels for one batch.\n","\n","    Returns:\n","        res: list. Accuracies corresponding to topk[0], topk[1], ...\n","    \"\"\"\n","    with torch.no_grad():\n","        maxk = max(topk)\n","        batch_size = target.size(0)\n","\n","        _, pred = output.topk(maxk, 1, True, True)\n","        pred = pred.t()\n","        correct = pred.eq(target.view(1, -1).expand_as(pred))\n","\n","        res = []\n","        for k in topk:\n","            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n","            res.append(correct_k.mul_(100.0 / batch_size))\n","        return res\n","\n","def test(model, dataloader):\n","\n","    model.eval()\n","\n","    top1_acc = []\n","    top5_acc = []\n","\n","    with torch.no_grad():\n","        for inputs, labels in dataloader:\n","            inputs = inputs.to(device)\n","            labels = labels.to(device)\n","\n","            outputs = model(inputs)\n","\n","            res = accuracy(outputs, labels, topk=(1, 5))\n","\n","            top1_acc.append(res[0] * len(outputs))\n","            top5_acc.append(res[1] * len(outputs))\n","\n","    print('Top-1 accuracy {}%, Top-5 accuracy {}%'.format(sum(top1_acc).item()/10000, sum(top5_acc).item()/10000))"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["##### To pass the test, both networks should have Top-5 accuracy above 55% #####\n","vgg_BN.load_state_dict(torch.load('./weights/MiniVGG-BN.pth'))\n","vgg.load_state_dict(torch.load('./weights/MiniVGG.pth'))\n","\n","test(vgg_BN, dataloaders['test'])\n","test(vgg, dataloaders['test'])"],"metadata":{"id":"6aCCQ7rXTlrj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 5.2 (a-b): Build small ResNet model (Optional)\n","\n","Please follow this figure to build the Residual Block and the Resnet model. We already implemented the back bone of the resnet model. You have to code the ResNet Block and the classifier part which are shown in the figure.\n","\n","__ResNet architecture__\n","\n","![alt text](https://drive.google.com/uc?id=1YFECibK--zL2jrG9Bodgj_38EA_2J79_)\n","\n"],"metadata":{"id":"R47KBeKyFUhG"}},{"cell_type":"code","source":["\n","class ResidualBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels, stride = 1):\n","        super(ResidualBlock, self).__init__()\n","        ###########################################################################\n","        # TODO: Code the residual block as depicted in the above figure. You should use    #\n","        # nn.Sequential().                                                        #\n","        ###########################################################################\n","\n","\n","\n","\n","        #######################################################################\n","        #                              END OF YOUR CODE                       #\n","        #######################################################################\n","\n","    def forward(self, x):\n","\n","        ###########################################################################\n","        # TODO: Code the forward pass for the residual block as depicted in the above figure.\n","        # Note: The relu activation function is after the skip connection.                                                        #\n","        ###########################################################################\n","\n","\n","\n","\n","        #######################################################################\n","        #                              END OF YOUR CODE                       #\n","        #######################################################################\n","        return out"],"metadata":{"id":"5M-2pmYR5byP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","class ResNet(nn.Module):\n","    def __init__(self, block, layers, num_classes = 10):\n","        super(ResNet, self).__init__()\n","\n","        ###########################################################################\n","        # Construct the neural net architecture for the resnet model. You should use nn.Sequential().\n","        # Note: We already implemented most of the network you just need to code the initial layers and insert the residual blocks.\n","        ###########################################################################\n","\n","        self.backbone = nn.Sequential(\n","                        ###########################################################################\n","                        # TODO: Code the initial layers i.e the the strided convolution layer, batchnorm, relu, maxpool layer and the residual blocks\n","                        #Hint: you have to make use of the \"block\" variable.\n","                        ###########################################################################\n","\n","\n","\n","                        #######################################################################\n","                        #                              END OF YOUR CODE                       #\n","                        #######################################################################\n","                        nn.Sequential(\n","                        nn.Conv2d(64, 128, kernel_size = 3, stride = 2, padding = 1),\n","                        nn.BatchNorm2d(128),\n","                        nn.ReLU()),\n","                        ###########################################################################\n","                        # TODO: Insert the residual block please follow the figure for the number of channels and the flow of layers.\n","                        ###########################################################################\n","\n","\n","                        #######################################################################\n","                        #                              END OF YOUR CODE                       #\n","                        #######################################################################\n","                        nn.Sequential(\n","                        nn.Conv2d(128, 256, kernel_size = 3, stride = 2, padding = 1),\n","                        nn.BatchNorm2d(256),\n","                        nn.ReLU()),\n","                        ###########################################################################\n","                        # TODO: Insert the residual block please follow the figure for the number of channels and the flow of layers.\n","                        ###########################################################################\n","\n","\n","                        #######################################################################\n","                        #                              END OF YOUR CODE                       #\n","                        #######################################################################\n","                        nn.Sequential(\n","                        nn.Conv2d(256, 512, kernel_size = 3, stride = 2, padding = 1),\n","                        nn.BatchNorm2d(512),\n","                        nn.ReLU()),\n","                        ###########################################################################\n","                        # TODO: Insert the residual block please follow the figure for the number of channels and the flow of layers.\n","                        ###########################################################################\n","\n","\n","\n","                        #######################################################################\n","                        #                              END OF YOUR CODE                       #\n","                        #######################################################################\n","\n","\n","        self.avgpool = nn.AvgPool2d(2, stride=1)\n","        self.fc = nn.Linear(512, num_classes)\n","\n","\n","    def forward(self, x):\n","        x = self.backbone(x)\n","        x = self.avgpool(x)\n","        x = x.view(x.size(0), -1)\n","        x = self.fc(x)\n","\n","        return x"],"metadata":{"id":"IgwJ_LgHpQU0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","resnet = ResNet(ResidualBlock, [1, 1, 1, 1], 100)\n","print(resnet)\n","print('Number of trainable parameters {}'.format(count_params(resnet)))\n"],"metadata":{"id":"TQu33eb4TIfw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 5.2 (c): Train ResNet (Optional)"],"metadata":{"id":"Wnjg0qbvX2os"}},{"cell_type":"code","source":["\n","# Initialize ResNet\n","resnet = ResNet(ResidualBlock, [1, 1, 1, 1], num_classes).to(device)\n","optimizer = make_optimizer(resnet)\n","\n","# Train the model!\n","resnet, tr_his_res, val_his_res = train_model(model=resnet, dataloaders=dataloaders, criterion=criterion, optimizer=optimizer,\n","           save_dir=save_dir, num_epochs=num_epochs, model_name='ResNet')\n","\n"],"metadata":{"id":"tuuBSa2-TiK8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# train/val accuracies for ResNet\n","plt.figure()\n","plt.plot(x, torch.tensor(tr_his_res, device = 'cpu'))\n","plt.plot(x, torch.tensor(val_his_res, device = 'cpu'))\n","plt.legend(['Training top1 accuracy', 'Validation top1 accuracy'])\n","plt.xticks(x)\n","plt.xlabel('Epoch')\n","plt.ylabel('Top1 Accuracy')\n","plt.title('ResNet')\n","plt.show()"],"metadata":{"id":"KZd0BvRQX-H2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pickle.dump(tr_his_res, open('tr_his_res.pkl', 'wb'))\n","pickle.dump(val_his_res, open('val_his_res.pkl', 'wb'))"],"metadata":{"id":"9vTm4eGJHzMA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["##### (Optional) Network should have Top-5 accuracy above 55% #####\n","resnet.load_state_dict(torch.load('./weights/ResNet.pth'))\n","test(resnet, dataloaders['test'])"],"metadata":{"id":"-miV-g_7TmrQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FyxHUcpoi8oI"},"source":["# Convert Notebook to PDF\n"]},{"cell_type":"code","source":["# generate pdf\n","# Please provide the full path of the notebook file below\n","# Important: make sure that your file name does not contain spaces!\n","import os\n","notebookpath = '' # Ex: notebookpath = '/content/drive/My Drive/Colab Notebooks/EECS 442 Fall 2023 - PS1.ipynb'\n","drive_mount_point = '/content/drive/'\n","from google.colab import drive\n","drive.mount(drive_mount_point)\n","file_name = notebookpath.split('/')[-1]\n","get_ipython().system(\"apt update && apt install texlive-xetex texlive-fonts-recommended texlive-generic-recommended\")\n","get_ipython().system(\"pip install pypandoc\")\n","get_ipython().system(\"apt-get install texlive texlive-xetex texlive-latex-extra pandoc\")\n","get_ipython().system(\"jupyter nbconvert --to PDF {}\".format(notebookpath.replace(' ', '\\\\ ')))\n","from google.colab import files\n","files.download(notebookpath.split('.')[0]+'.pdf')"],"metadata":{"id":"I3-ciFonnPlz"},"execution_count":null,"outputs":[]}]}