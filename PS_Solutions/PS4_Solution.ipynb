{"cells":[{"cell_type":"markdown","metadata":{"id":"ix5dQS2rUMlu"},"source":["#EECS 442 PS4: Backpropagation\n","\n","__Please provide the following information__\n","(e.g. Andrew Owens, ahowens):\n","\n","[Your first name] [Your last name], [Your UMich uniqname]\n","\n","__Important__: after you download the .ipynb file, please name it as __\\<your_uniquename\\>_\\<your_umid\\>.ipynb__ before you submit it to canvas. Example: adam_01101100.ipynb.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"W_Cst4k4tuBc"},"source":["# Starting\n","\n","Run the following code to import the modules you'll need. After your finish the assignment, remember to run all cells and save the note book to your local machine as a .ipynb file for Canvas submission."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10221,"status":"ok","timestamp":1696412180633,"user":{"displayName":"Yueqi Wu","userId":"16342215818375271323"},"user_tz":-480},"id":"SHumIO-xt57H","outputId":"6df23512-a0f0-4082-d848-a4ec9f3796b6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./cifar-10-python.tar.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 170498071/170498071 [00:02<00:00, 56934370.23it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./cifar-10-python.tar.gz to .\n"]}],"source":["import pickle\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import os\n","import math\n","from torchvision.datasets import CIFAR10\n","download = not os.path.isdir('cifar-10-batches-py')\n","dset_train = CIFAR10(root='.', download=download)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"87aUvJJ52FeY"},"outputs":[],"source":["import torch"]},{"cell_type":"markdown","metadata":{"id":"4WpKb7SvKR6W"},"source":["# Problem 4.1 Understanding Backpropagation"]},{"cell_type":"markdown","metadata":{"id":"Fy-7c_kJKUCd"},"source":["# 4.1 (b)  \n","Implement the code for forward and backward pass of computation graph in (a)"]},{"cell_type":"markdown","metadata":{"id":"vxx6rXS39lYU"},"source":["###solution"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yamHzJWpKS94"},"outputs":[],"source":["def f_1(x0, x1, x2, w0, w1, w2, w3):\n","    \"\"\"\n","    Computes the forward and backward pass through the computational graph\n","    of (a)\n","\n","    Inputs:\n","    - x0, x1, w0, w1, w2: Python floats\n","\n","    Returns a tuple of:\n","    - L: The output of the graph\n","    - grads: A tuple (grad_x0, grad_x1, grad_w0, grad_w1, grad_w2)\n","    giving the derivative of the output L with respect to each input.\n","    \"\"\"\n","    ###########################################################################\n","    # TODO: Implement the forward pass for the computational graph for (a) and#\n","    # store the output of this graph as L                                     #\n","    ###########################################################################\n","\n","    ###########################################################################\n","    #                              END OF YOUR CODE                           #\n","    ###########################################################################\n","    a = w0 * x0\n","    d = 1.0 / x1\n","    b = d * w1\n","    c = w2 * x2\n","    s1 = a + b + w3\n","    s2 = s1 - c\n","    e = s2 * (-1)\n","    f = math.exp(e)\n","    g = f + 1\n","    L = 1.0 / g\n","\n","    ###########################################################################\n","    # TODO: Implement the backward pass for the computational graph for (a)   #\n","    # Store the gradients for each input                                      #\n","    ###########################################################################\n","    grad_L = 1\n","    grad_g = grad_L * (-1/(g**2))\n","    grad_f = grad_g * 1\n","    grad_e = grad_f * math.exp(e)\n","    grad_s2 = -1 * grad_e\n","    grad_c = -1 * grad_s2\n","    grad_s1 = grad_s2\n","    grad_a = grad_s1\n","    grad_b = grad_s1\n","    grad_d = w1 * grad_b\n","\n","    grad_w3 = grad_s1\n","\n","    grad_w0 = x0 * grad_a\n","    grad_x0 = w0 * grad_a\n","\n","    grad_w1 = d * grad_b\n","    grad_x1 = grad_d * (-1/(x1**2))\n","\n","    grad_w2 = x2 * grad_c\n","    grad_x2 = w2 * grad_c\n","    ###########################################################################\n","    #                              END OF YOUR CODE                           #\n","    ###########################################################################\n","\n","    grads = (grad_x0, grad_x1, grad_x2, grad_w0, grad_w1, grad_w2, grad_w3)\n","    return L, grads"]},{"cell_type":"markdown","metadata":{"id":"nnbct8nD8Luo"},"source":["###pytorch version for test\n","only to test the above solution is correct or not, but using pytorch version still need to implement the forward pass"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9PdO1yuI8LUQ"},"outputs":[],"source":["def f_1_torch(x0, x1, x2, w0, w1, w2, w3):\n","    \"\"\"\n","    Computes the forward and backward pass through the computational graph\n","    of (a)\n","\n","    Inputs:\n","    - x0, x1, x2, w0, w1, w2, w3: Python floats\n","\n","    Returns a tuple of:\n","    - L: The output of the graph\n","    - grads: A tuple (grad_x0, grad_x1, grad_x2, grad_w0, grad_w1, grad_w2, grad_w3)\n","    giving the derivative of the output L with respect to each input.\n","    \"\"\"\n","    ###########################################################################\n","    # TODO: Implement the forward pass for the computational graph for (a) and#\n","    # store the output of this graph as L                                     #\n","    ###########################################################################\n","\n","    ###########################################################################\n","    #                              END OF YOUR CODE                           #\n","    ###########################################################################\n","\n","    x0 = torch.tensor(x0, requires_grad=True)\n","    x1 = torch.tensor(x1, requires_grad=True)\n","    x2 = torch.tensor(x2, requires_grad=True)\n","    w0 = torch.tensor(w0, requires_grad=True)\n","    w1 = torch.tensor(w1, requires_grad=True)\n","    w2 = torch.tensor(w2, requires_grad=True)\n","    w3 = torch.tensor(w3, requires_grad=True)\n","\n","    a = w0 * x0\n","    d = 1.0 / x1\n","    b = d * w1\n","    c = w2 * x2\n","    s1 = a + b + w3\n","    s2 = s1 - c\n","    e = s2 * (-1)\n","    f = torch.exp(e)\n","    g = f + 1\n","    L = 1.0 / g\n","\n","    ###########################################################################\n","    # TODO: Implement the backward pass for the computational graph for (a)   #\n","    # Store the gradients for each input                                      #\n","    ###########################################################################\n","    external_grad = torch.tensor(1.)\n","    L.backward(gradient=external_grad)\n","    grad_x0 = x0.grad\n","    grad_x1 = x1.grad\n","    grad_x2 = x2.grad\n","    grad_w0 = w0.grad\n","    grad_w1 = w1.grad\n","    grad_w2 = w2.grad\n","    grad_w3 = w3.grad\n","    ###########################################################################\n","    #                              END OF YOUR CODE                           #\n","    ###########################################################################\n","\n","    grads = (grad_x0, grad_x1, grad_x2, grad_w0, grad_w1, grad_w2, grad_w3)\n","    return L, grads"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":268,"status":"ok","timestamp":1696436939800,"user":{"displayName":"Sarah Jabbour","userId":"04596287297174769370"},"user_tz":240},"id":"bJtgUuM34qvS","outputId":"94dbfe77-0f5c-4c89-ffee-a6e36f82c206"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(0.7744014530142785,\n"," (0.0873519212918263,\n","  -0.2426442258106286,\n","  -0.0873519212918263,\n","  0.0873519212918263,\n","  0.29117307097275436,\n","  -0.12229268980855681,\n","  0.1747038425836526))"]},"metadata":{},"execution_count":5}],"source":["f_1(0.5, 0.6, 0.7, 0.5, 0.5, 0.5, 0.5)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":248,"status":"ok","timestamp":1696436941892,"user":{"displayName":"Sarah Jabbour","userId":"04596287297174769370"},"user_tz":240},"id":"FFMo4AXp8haT","outputId":"8268947d-ad3f-4a3f-abe7-3545e8319945"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor(0.7744, grad_fn=<MulBackward0>),\n"," (tensor(0.0874),\n","  tensor(-0.2426),\n","  tensor(-0.0874),\n","  tensor(0.0874),\n","  tensor(0.2912),\n","  tensor(-0.1223),\n","  tensor(0.1747)))"]},"metadata":{},"execution_count":6}],"source":["f_1_torch(0.5, 0.6, 0.7, 0.5, 0.5, 0.5, 0.5)"]},{"cell_type":"markdown","metadata":{"id":"CdquTNqGKYcc"},"source":["# 4.1 (c)  \n","Implement the code for forward and backward pass of computation graph in (c)\n"]},{"cell_type":"markdown","metadata":{"id":"7sQEwgGX9q8y"},"source":["###solution"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o55wTks0KaPC"},"outputs":[],"source":["def f_2(x, y, z, w):\n","    \"\"\"\n","    Computes the forward and backward pass through the computational graph\n","    of (c)\n","\n","    Inputs:\n","    - x, y, z: Python floats\n","\n","    Returns a tuple of:\n","    - L: The output of the graph\n","    - grads: A tuple (grad_x, grad_y, grad_z)\n","    giving the derivative of the output L with respect to each input.\n","    \"\"\"\n","    ###########################################################################\n","    # TODO: Implement the forward pass for the computational graph for (c) and#\n","    # store the output of this graph as L                                     #\n","    ###########################################################################\n","    a = 1.0 / w\n","    b = -1 * x\n","    c = math.exp(y)\n","    d = math.exp(z)\n","    p = c / d\n","    e = a**b\n","    f = p * c\n","    g = 1.0 / p\n","    m = e - f\n","    n = m + g\n","    L = 1.0 / n\n","\n","    ###########################################################################\n","    #                              END OF YOUR CODE                           #\n","    ###########################################################################\n","\n","\n","    ###########################################################################\n","    # TODO: Implement the backward pass for the computational graph for (c)   #\n","    # Store the gradients for each input                                      #\n","    ###########################################################################\n","    grad_L = 1\n","    grad_n = grad_L * (-1/(n**2))\n","    grad_m = grad_n\n","    grad_g = grad_n\n","    grad_e = grad_m\n","    grad_b = a**b * math.log(a) * grad_e\n","    grad_a = b * a**(b-1)  * grad_e\n","    grad_f = -1 * grad_m\n","    # grad_c = grad_f * p\n","    grad_p = grad_f * c + grad_g * (-1) / (p**2)\n","    grad_c = grad_f * p + grad_p * 1.0 / d\n","    grad_d = grad_p * (-c/d**2)\n","\n","    grad_w = grad_a * (-1/(w**2))\n","    grad_x = -1 * grad_b\n","    grad_y = grad_c * math.exp(y)\n","    grad_z = grad_d * math.exp(z)\n","    ###########################################################################\n","    #                              END OF YOUR CODE                           #\n","    ###########################################################################\n","\n","    grads = (grad_x, grad_y, grad_z, grad_w)\n","    return L, grads"]},{"cell_type":"markdown","metadata":{"id":"Z1xUEaaB9Emb"},"source":["###pytorch version for test\n","only to test the above solution is correct or not, using pytorch version still need to implement the forward pass"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rmW5_j5P9Fos"},"outputs":[],"source":["def f_2_torch(x, y, z, w):\n","    \"\"\"\n","    Computes the forward and backward pass through the computational graph\n","    of (c)\n","\n","    Inputs:\n","    - x, y, z, w: Python floats\n","\n","    Returns a tuple of:\n","    - L: The output of the graph\n","    - grads: A tuple (grad_x, grad_y, grad_z, grad_w)\n","    giving the derivative of the output L with respect to each input.\n","    \"\"\"\n","    ###########################################################################\n","    # TODO: Implement the forward pass for the computational graph for (c) and#\n","    # store the output of this graph as L                                     #\n","    ###########################################################################\n","    x = torch.tensor(x, requires_grad=True)\n","    y = torch.tensor(y, requires_grad=True)\n","    z = torch.tensor(z, requires_grad=True)\n","    w = torch.tensor(w, requires_grad=True)\n","\n","    ###########################################################################\n","    #                              END OF YOUR CODE                           #\n","    ###########################################################################\n","    a = 1.0 / w\n","    b = -1 * x\n","    c = math.exp(y)\n","    d = math.exp(z)\n","    p = c / d\n","    e = a**b\n","    f = p * c\n","    g = 1.0 / p\n","    m = e - f\n","    n = m + g\n","    L = 1.0 / n\n","\n","    ###########################################################################\n","    # TODO: Implement the backward pass for the computational graph for (c)   #\n","    # Store the gradients for each input                                      #\n","    ###########################################################################\n","    external_grad = torch.tensor(1.)\n","    L.backward(gradient=external_grad)\n","    grad_x = x.grad\n","    grad_y = y.grad\n","    grad_z = z.grad\n","    grad_w = w.grad\n","    ###########################################################################\n","    #                              END OF YOUR CODE                           #\n","    ###########################################################################\n","\n","    grads = (grad_x, grad_y, grad_z, grad_w)\n","    return L, grads"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":632,"status":"ok","timestamp":1696412185286,"user":{"displayName":"Yueqi Wu","userId":"16342215818375271323"},"user_tz":-480},"id":"iUk0t2YU-ByZ","outputId":"5bb293bd-8f09-493a-8175-734fb1393a01"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(2.8500028802984354,\n"," (1.621137798397557,\n","  35.76030010542131,\n","  -17.758039491522613,\n","  -4.54062471458906))"]},"metadata":{},"execution_count":6}],"source":["f_2(0.5, 0.6, 0.7, 0.8)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":407,"status":"ok","timestamp":1696412209895,"user":{"displayName":"Yueqi Wu","userId":"16342215818375271323"},"user_tz":-480},"id":"pRTv-ejW-HVc","outputId":"038db336-9ed2-4c52-8651-d92167fe7715"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor(2.8500, grad_fn=<MulBackward0>),\n"," (tensor(1.6211), None, None, tensor(-4.5406)))"]},"metadata":{},"execution_count":9}],"source":["f_2_torch(0.5, 0.6, 0.7, 0.8)"]},{"cell_type":"markdown","metadata":{"id":"apEPzDNtK0MC"},"source":["# Problem 4.2 Softmax Classifier with Two Layer Neural Network\n","In this problem you will develop a two Layer neural network with fully-connected layers to perform classification, and test it out on the CIFAR-10 dataset.\n","\n","We train the network with a softmax loss function on the weight matrices. The network uses a ReLU nonlinearity after the first fully connected layer. In other words, the network has the following architecture:\n","\n","input - fully connected layer - ReLU - fully connected layer - softmax\n","\n","The outputs of the second fully-connected layer are the scores for each class.\n","\n","You cannot use any deep learning libraries such as PyTorch in this part."]},{"cell_type":"markdown","metadata":{"id":"SXfumCQ21JoK"},"source":["# 4.2 (a) Layers\n","In this problem, implement fully connected layer, relu and softmax. Filling in all TODOs in skeleton codes will be sufficient."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q-ljfgMv9PHx"},"outputs":[],"source":["def fc_forward(X, W, b):\n","    \"\"\"\n","    Computes the forward pass for a fully-connected layer.\n","\n","    The input X has shape (N, Din) and contains a minibatch of N\n","    examples, where each example x[i] has shape (Din,).\n","\n","    Inputs:\n","    - X: A numpy array containing input data, of shape (N, Din)\n","    - W: A numpy array of weights, of shape (Din, Dout)\n","    - b: A numpy array of biases, of shape (Dout,)\n","\n","    Returns a tuple of:\n","    - out: output, of shape (N, Dout)\n","    - cache: (X, W, b)\n","    \"\"\"\n","    ###########################################################################\n","    # TODO: Implement the forward pass. Store the result in out.              #\n","    ###########################################################################\n","    out = np.dot(X,W) + b\n","    ###########################################################################\n","    #                             END OF YOUR CODE                            #\n","    ###########################################################################\n","    cache = (X, W, b)\n","    return out, cache\n","\n","\n","def fc_backward(dout, cache):\n","    \"\"\"\n","    Computes the backward pass for a fully_connected layer.\n","\n","    Inputs:\n","    - dout: Upstream derivative, of shape (N, Dout)\n","    - cache: returned by your forward function. Tuple of:\n","      - X: Input data, of shape (N, Din)\n","      - W: Weights, of shape (Din, Dout)\n","      - b: Biases, of shape (Dout,)\n","\n","    Returns a tuple of:\n","    - dX: Gradient with respect to X, of shape (N, Din)\n","    - dW: Gradient with respect to W, of shape (Din, Dout)\n","    - db: Gradient with respect to b, of shape (Dout,)\n","    \"\"\"\n","    X, W, b = cache\n","    dX, dW, db = None, None, None\n","    ###########################################################################\n","    # TODO: Implement the affine backward pass.                               #\n","    ###########################################################################\n","    dX = np.dot(dout,W.T)\n","    dW = np.dot(X.T, dout)\n","    db = dout.sum(axis=0)\n","    ###########################################################################\n","    #                             END OF YOUR CODE                            #\n","    ###########################################################################\n","    return dX, dW, db\n","\n","def relu_forward(x):\n","    \"\"\"\n","    Computes the forward pass for a layer of rectified linear units (ReLUs).\n","\n","    Input:\n","    - x: Inputs, of any shape\n","\n","    Returns a tuple of:\n","    - out: Output, of the same shape as x\n","    - cache: x\n","    \"\"\"\n","    out = x.copy()\n","    ###########################################################################\n","    # TODO: Implement the ReLU forward pass.                                  #\n","    ###########################################################################\n","    out[out < 0] = 0\n","    # out = np.maximum(0, x)\n","    ###########################################################################\n","    #                             END OF YOUR CODE                            #\n","    ###########################################################################\n","    cache = x\n","\n","    return out, cache\n","\n","\n","def relu_backward(dout, cache):\n","    \"\"\"\n","    Computes the backward pass for a layer of rectified linear units (ReLUs).\n","\n","    Input:\n","    - dout: Upstream derivatives, of any shape\n","    - cache: returned by your forward function. Input x, of same shape as dout\n","\n","    Returns:\n","    - dx: Gradient with respect to x\n","    \"\"\"\n","    dx, x = dout.copy(), cache\n","    ###########################################################################\n","    # TODO: Implement the ReLU backward pass.                                 #\n","    ###########################################################################\n","    dx[x <= 0] = 0\n","    ###########################################################################\n","    #                             END OF YOUR CODE                            #\n","    ###########################################################################\n","    return dx\n","\n","\n","def softmax_loss(X, y):\n","    \"\"\"\n","    Computes the loss and gradient for softmax classification.\n","\n","    Inputs:\n","    - X: Input data, of shape (N, C) where x[i, j] is the score for the jth\n","      class for the ith input.\n","    - y: Vector of labels, of shape (N,) where y[i] is the label for X[i] and\n","      0 <= y[i] < C\n","\n","    Returns a tuple of:\n","    - loss: Scalar giving the loss\n","    - dX: Gradient of the loss with respect to x\n","    \"\"\"\n","    loss, dX = None, None\n","\n","    dX = np.exp(X - np.max(X, axis=1, keepdims=True))\n","    dX /= np.sum(dX, axis=1, keepdims=True)\n","    loss = -np.sum(np.log(dX[np.arange(X.shape[0]), y])) / X.shape[0]\n","    dX[np.arange(X.shape[0]), y] -= 1\n","    dX /= X.shape[0]\n","\n","\n","    return loss, dX"]},{"cell_type":"markdown","metadata":{"id":"LbFxtS3zK8oz"},"source":["# 4.2 (b) Two Layer Softmax Classifier\n","\n","In this problem, implement two layer softmax classifier."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ytvxbx9UpxVL"},"outputs":[],"source":["class SoftmaxClassifier(object):\n","    \"\"\"\n","    A fully-connected neural network with\n","    softmax loss that uses a modular layer design. We assume an input dimension\n","    of D, a hidden dimension of H, and perform classification over C classes.\n","\n","    The architecture should be fc - relu - fc - softmax with one hidden layer\n","\n","    The learnable parameters of the model are stored in the dictionary\n","    self.params that maps parameter names to numpy arrays.\n","    \"\"\"\n","\n","    def __init__(self, input_dim=3072, hidden_dim=300, num_classes=10,\n","                 weight_scale=1e-3):\n","        \"\"\"\n","        Initialize a new network.\n","\n","        Inputs:f\n","        - input_dim: An integer giving the size of the input\n","        - hidden_dim: An integer giving the size of the hidden layer, None\n","          if there's no hidden layer.\n","        - num_classes: An integer giving the number of classes to classify\n","        - weight_scale: Scalar giving the standard deviation for random\n","          initialization of the weights.\n","        \"\"\"\n","        self.params = {}\n","        ############################################################################\n","        # TODO: Initialize the weights and biases of the two-layer net. Weights    #\n","        # should be initialized from a Gaussian centered at 0.0 with               #\n","        # standard deviation equal to weight_scale, and biases should be           #\n","        # initialized to zero. All weights and biases should be stored in the      #\n","        # dictionary self.params, with fc weights and biases using the keys        #\n","        # 'W' and 'b', i.e., W1, b1 for the weights and bias in the first linear   #\n","        # layer, W2, b2 for the weights and bias in the second linear layer.       #\n","        ############################################################################\n","        self.hidden_dim = hidden_dim\n","        self.params['W1'] = np.random.normal(scale=weight_scale, size=(input_dim, hidden_dim))\n","        self.params['b1'] = np.zeros(hidden_dim)\n","        self.params['W2'] = np.random.normal(scale=weight_scale, size=(hidden_dim, num_classes))\n","        self.params['b2'] = np.zeros(num_classes)\n","        ############################################################################\n","        #                             END OF YOUR CODE                             #\n","        ############################################################################\n","\n","\n","    def forwards_backwards(self, X, y=None):\n","        \"\"\"\n","        Compute loss and gradient for a minibatch of data.\n","\n","        Inputs:\n","        - X: Array of input data of shape (N, Din)\n","        - y: Array of labels, of shape (N,). y[i] gives the label for X[i].\n","\n","        Returns:\n","        If y is None, then run a test-time forward pass of the model and return:\n","        - scores: Array of shape (N, C) giving classification scores, where\n","          scores[i, c] is the classification score for X[i] and class c.\n","\n","        If y is not None, then run a training-time forward and backward pass. And\n","        return a tuple of:\n","        - loss: Scalar value giving the loss\n","        - grads: Dictionary with the same keys as self.params, mapping parameter\n","          names to gradients of the loss with respect to those parameters.\n","        \"\"\"\n","        scores = None\n","        ############################################################################\n","        # TODO: Implement the forward pass for the two-layer net, computing the    #\n","        # class scores for X and storing them in the scores variable.              #\n","        ############################################################################\n","        out_1, cache_1 = fc_forward(X, self.params['W1'], self.params['b1'])\n","        out_2, cache_2 = relu_forward(out_1)\n","        scores, cache_3 = fc_forward(out_2, self.params['W2'], self.params['b2'])\n","        ############################################################################\n","        #                             END OF YOUR CODE                             #\n","        ############################################################################\n","\n","        # If y is None then we are in test mode so just return scores\n","        if y is None:\n","            return scores\n","\n","        loss, grads = 0, {}\n","        ############################################################################\n","        # TODO: Implement the backward pass for the two-layer net. Store the loss  #\n","        # in the loss variable and gradients in the grads dictionary. Compute data #\n","        # loss using softmax, and make sure that grads[k] holds the gradients for  #\n","        # self.params[k].                                                          #\n","        ############################################################################\n","        loss, dX = softmax_loss(scores, y)\n","        dX_1, grads['W2'], grads['b2'] = fc_backward(dX, cache_3)\n","        dX_2 = relu_backward(dX_1,cache_2)\n","        _, grads['W1'], grads['b1'] = fc_backward(dX_2, cache_1)\n","        ############################################################################\n","        #                             END OF YOUR CODE                             #\n","        ############################################################################\n","        return loss, grads\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"lwp0waIL1h_e"},"source":["# 4.2(c) Training\n","\n","In this problem, you need to preprocess the images and set up model hyperparameters. Notice that adjust the training and val split is optional."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":111764,"status":"ok","timestamp":1696294673142,"user":{"displayName":"Yiming Dou","userId":"00198832258301430987"},"user_tz":240},"id":"kZPtQzXGMoCg","outputId":"689bdbd4-bf55-41b8-acbd-8666a635cfc5"},"outputs":[{"output_type":"stream","name":"stdout","text":["(Iteration 1 / 3120) loss: 2.303348\n","(Epoch 0 / 10) train acc: 0.168000; val_acc: 0.169500\n","(Epoch 1 / 10) train acc: 0.471000; val_acc: 0.434700\n","(Epoch 2 / 10) train acc: 0.486000; val_acc: 0.453800\n","(Epoch 3 / 10) train acc: 0.528000; val_acc: 0.487900\n","(Iteration 1001 / 3120) loss: 1.340807\n","(Epoch 4 / 10) train acc: 0.555000; val_acc: 0.498800\n","(Epoch 5 / 10) train acc: 0.593000; val_acc: 0.501800\n","(Epoch 6 / 10) train acc: 0.620000; val_acc: 0.514700\n","(Iteration 2001 / 3120) loss: 1.078427\n","(Epoch 7 / 10) train acc: 0.631000; val_acc: 0.518100\n","(Epoch 8 / 10) train acc: 0.688000; val_acc: 0.522900\n","(Epoch 9 / 10) train acc: 0.694000; val_acc: 0.524500\n","(Iteration 3001 / 3120) loss: 0.969423\n","(Epoch 10 / 10) train acc: 0.721000; val_acc: 0.525500\n"]}],"source":["def unpickle(file):\n","    with open(file, 'rb') as fo:\n","        dict = pickle.load(fo, encoding=\"latin1\")\n","    return dict\n","\n","def load_cifar10():\n","    data = {}\n","    meta = unpickle(\"cifar-10-batches-py/batches.meta\")\n","    batch1 = unpickle(\"cifar-10-batches-py/data_batch_1\")\n","    batch2 = unpickle(\"cifar-10-batches-py/data_batch_2\")\n","    batch3 = unpickle(\"cifar-10-batches-py/data_batch_3\")\n","    batch4 = unpickle(\"cifar-10-batches-py/data_batch_4\")\n","    batch5 = unpickle(\"cifar-10-batches-py/data_batch_5\")\n","    test_batch = unpickle(\"cifar-10-batches-py/test_batch\")\n","    X_train = np.vstack((batch1['data'], batch2['data'], batch3['data'],\\\n","                         batch4['data'], batch5['data']))\n","    Y_train = np.array(batch1['labels'] + batch2['labels'] + batch3['labels'] +\n","                       batch4['labels'] + batch5['labels'])\n","    X_test = test_batch['data']\n","    Y_test = test_batch['labels']\n","\n","    #Preprocess images here\n","    X_train = (X_train-np.mean(X_train,axis=1,keepdims=True))/np.std(X_train,axis=1,keepdims=True)\n","    X_test = (X_test-np.mean(X_test,axis=1,keepdims=True))/np.std(X_test,axis=1,keepdims=True)\n","\n","    data['X_train'] = X_train[:40000]\n","    data['y_train'] = Y_train[:40000]\n","    data['X_val'] = X_train[40000:]\n","    data['y_val'] = Y_train[40000:]\n","    data['X_test'] = X_test\n","    data['y_test'] = Y_test\n","    return data\n","\n","def testNetwork(model, X, y, num_samples=None, batch_size=100):\n","    \"\"\"\n","    Check accuracy of the model on the provided data.\n","\n","    Inputs:\n","    - model: Image classifier\n","    - X: Array of data, of shape (N, d_1, ..., d_k)\n","    - y: Array of labels, of shape (N,)\n","    - num_samples: If not None, subsample the data and only test the model\n","      on num_samples datapoints.\n","    - batch_size: Split X and y into batches of this size to avoid using\n","      too much memory.\n","\n","    Returns:\n","    - acc: Scalar giving the fraction of instances that were correctly\n","      classified by the model.\n","    \"\"\"\n","\n","    # Subsample the data\n","    N = X.shape[0]\n","    if num_samples is not None and N > num_samples:\n","        mask = np.random.choice(N, num_samples)\n","        N = num_samples\n","        X = X[mask]\n","        y = y[mask]\n","\n","    # Compute predictions in batches\n","    num_batches = N // batch_size\n","    if N % batch_size != 0:\n","        num_batches += 1\n","    y_pred = []\n","    for i in range(num_batches):\n","        start = i * batch_size\n","        end = (i + 1) * batch_size\n","        scores = model.forwards_backwards(X[start:end])\n","        y_pred.append(np.argmax(scores, axis=1))\n","    y_pred = np.hstack(y_pred)\n","    acc = np.mean(y_pred == y)\n","\n","    return acc\n","\n","def SGD(W,dW, learning_rate=1e-3):\n","    \"\"\" Apply a gradient descent step on weight W\n","    Inputs:\n","        W : Weight matrix\n","        dW : gradient of weight, same shape as W\n","        learning_rate : Learning rate. Defaults to 1e-3.\n","    Returns:\n","        new_W: Updated weight matrix\n","    \"\"\"\n","\n","    # Apply a gradient descent step on weight W using the gradient dW and the specified learning rate.\n","    new_W = W - learning_rate * dW\n","\n","    return new_W\n","\n","def trainNetwork(model, data, **kwargs):\n","    \"\"\"\n","     Required arguments:\n","    - model: Image classifier\n","    - data: A dictionary of training and validation data containing:\n","      'X_train': Array, shape (N_train, d_1, ..., d_k) of training images\n","      'X_val': Array, shape (N_val, d_1, ..., d_k) of validation images\n","      'y_train': Array, shape (N_train,) of labels for training images\n","      'y_val': Array, shape (N_val,) of labels for validation images\n","\n","    Optional arguments:\n","    - learning_rate: A scalar for initial learning rate.\n","    - lr_decay: A scalar for learning rate decay; after each epoch the\n","      learning rate is multiplied by this value.\n","    - batch_size: Size of minibatches used to compute loss and gradient\n","      during training.\n","    - num_epochs: The number of epochs to run for during training.\n","    - print_every: Integer; training losses will be printed every\n","      print_every iterations.\n","    - verbose: Boolean; if set to false then no output will be printed\n","      during training.\n","    - num_train_samples: Number of training samples used to check training\n","      accuracy; default is 1000; set to None to use entire training set.\n","    - num_val_samples: Number of validation samples to use to check val\n","      accuracy; default is None, which uses the entire validation set.\n","    - optimizer: Choice of using either 'SGD' or 'SGD_Momentum' for updating weights; default is SGD.\n","    \"\"\"\n","\n","\n","    learning_rate =  kwargs.pop('learning_rate', 1e-3)\n","    lr_decay = kwargs.pop('lr_decay', 1.0)\n","    batch_size = kwargs.pop('batch_size', 100)\n","    num_epochs = kwargs.pop('num_epochs', 10)\n","    num_train_samples = kwargs.pop('num_train_samples', 1000)\n","    num_val_samples = kwargs.pop('num_val_samples', None)\n","    print_every = kwargs.pop('print_every', 10)\n","    verbose = kwargs.pop('verbose', True)\n","    optimizer = kwargs.pop('optimizer', 'SGD')\n","\n","    epoch = 0\n","    best_val_acc = 0\n","    best_params = {}\n","    loss_history = []\n","    train_acc_history = []\n","    val_acc_history = []\n","\n","\n","    num_train = data['X_train'].shape[0]\n","    iterations_per_epoch = max(num_train // batch_size, 1)\n","    num_iterations = num_epochs * iterations_per_epoch\n","\n","    #Initialize velocity dictionary if optimizer is SGD_Momentum\n","    if optimizer == 'SGD_Momentum':\n","      velocity_dict = {p:np.zeros(w.shape) for p,w in model.params.items()}\n","\n","    for t in range(num_iterations):\n","        # Make a minibatch of training data\n","        batch_mask = np.random.choice(num_train, batch_size)\n","        X_batch = data['X_train'][batch_mask]\n","        y_batch = data['y_train'][batch_mask]\n","\n","        # Compute loss and gradient\n","        loss, grads = model.forwards_backwards(X_batch, y_batch)\n","        loss_history.append(loss)\n","\n","        # Perform a parameter update\n","        if optimizer == 'SGD':\n","          for p, w in model.params.items():\n","              model.params[p] = SGD(w,grads[p], learning_rate=learning_rate)\n","\n","        elif optimizer == 'SGD_Momentum':\n","          for p, w in model.params.items():\n","              model.params[p], velocity_dict[p] = SGD_Momentum(w, grads[p], velocity_dict[p], beta=0.5, learning_rate=learning_rate)\n","        else:\n","          raise NotImplementedError\n","        # Print training loss\n","        if verbose and t % print_every == 0:\n","            print('(Iteration %d / %d) loss: %f' % (\n","                   t + 1, num_iterations, loss_history[-1]))\n","\n","        # At the end of every epoch, increment the epoch counter and decay\n","        # the learning rate.\n","        epoch_end = (t + 1) % iterations_per_epoch == 0\n","        if epoch_end:\n","            epoch += 1\n","            learning_rate *= lr_decay\n","\n","        # Check train and val accuracy on the first iteration, the last\n","        # iteration, and at the end of each epoch.\n","        first_it = (t == 0)\n","        last_it = (t == num_iterations - 1)\n","        if first_it or last_it or epoch_end:\n","            train_acc = testNetwork(model, data['X_train'], data['y_train'],\n","                num_samples= num_train_samples)\n","            val_acc = testNetwork(model, data['X_val'], data['y_val'],\n","                num_samples=num_val_samples)\n","            train_acc_history.append(train_acc)\n","            val_acc_history.append(val_acc)\n","\n","            if verbose:\n","                print('(Epoch %d / %d) train acc: %f; val_acc: %f' % (\n","                       epoch, num_epochs, train_acc, val_acc))\n","\n","            # Keep track of the best model\n","            if val_acc > best_val_acc:\n","                best_val_acc = val_acc\n","                best_params = {}\n","                for k, v in model.params.items():\n","                    best_params[k] = v.copy()\n","\n","    model.params = best_params\n","\n","    return model, train_acc_history, val_acc_history\n","\n","\n","# load data\n","data = load_cifar10()\n","train_data = { k: data[k] for k in ['X_train', 'y_train',\n","                                    'X_val', 'y_val']}\n","#######################################################################\n","# TODO: Set up model hyperparameters for SGD                               #\n","#######################################################################\n","\n","# initialize model\n","model_SGD = SoftmaxClassifier(hidden_dim = 300, weight_scale=1e-2)\n","\n","# start training using SGD\n","model_SGD, train_acc_history_SGD, val_acc_history_SGD = trainNetwork(\n","    model_SGD, train_data,\n","    learning_rate = 0.05,\n","    lr_decay=0.9,\n","    batch_size=128,\n","    num_epochs=10,\n","     print_every=1000, optimizer = 'SGD')\n","#######################################################################\n","#                         END OF YOUR CODE                            #\n","#######################################################################\n"]},{"cell_type":"markdown","metadata":{"id":"e2ilTVXIw_7q"},"source":["# 4.2(d) Training with SGD_Momentum\n","\n","The model above was trained using SGD. Now implement the"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"54jGVPZOXtV6","executionInfo":{"status":"ok","timestamp":1696294806172,"user_tz":240,"elapsed":122081,"user":{"displayName":"Yiming Dou","userId":"00198832258301430987"}},"outputId":"cf3ddcdf-fc0f-4522-bc8d-09a7d0564739"},"outputs":[{"output_type":"stream","name":"stdout","text":["(Iteration 1 / 3120) loss: 2.306189\n","(Epoch 0 / 10) train acc: 0.124000; val_acc: 0.131500\n","(Epoch 1 / 10) train acc: 0.515000; val_acc: 0.440900\n","(Epoch 2 / 10) train acc: 0.503000; val_acc: 0.471900\n","(Epoch 3 / 10) train acc: 0.588000; val_acc: 0.489300\n","(Iteration 1001 / 3120) loss: 1.097247\n","(Epoch 4 / 10) train acc: 0.614000; val_acc: 0.490300\n","(Epoch 5 / 10) train acc: 0.648000; val_acc: 0.502200\n","(Epoch 6 / 10) train acc: 0.674000; val_acc: 0.505200\n","(Iteration 2001 / 3120) loss: 1.021200\n","(Epoch 7 / 10) train acc: 0.708000; val_acc: 0.512000\n","(Epoch 8 / 10) train acc: 0.722000; val_acc: 0.510500\n","(Epoch 9 / 10) train acc: 0.758000; val_acc: 0.512700\n","(Iteration 3001 / 3120) loss: 0.664793\n","(Epoch 10 / 10) train acc: 0.769000; val_acc: 0.515900\n"]}],"source":["def SGD_Momentum(W, dW, velocity, beta=0.5, learning_rate=1e-3):\n","    \"\"\" Apply a gradient descent with momentum update on weight W\n","    Inputs:\n","        W : Weight matrix\n","        dW : gradient of weight, same shape as W\n","        velocity : velocity matrix, same shape as W\n","        beta : scalar value in range [0,1] weighting the velocity matrix. Setting it to 0 should make SGD_Momentum same as SGD.\n","               Defaults to 0.5.\n","        learning_rate : Learning rate. Defaults to 1e-3.\n","    Returns:\n","        new_W: Updated weight matrix\n","        new_velocity: Updated velocity matrix\n","    \"\"\"\n","    # ===== your code here! =====\n","    # TODO:\n","    # Apply a gradient descent step on weight W using the gradient dW and the specified learning rate.\n","    # 1. Calculate the new velocity by using the velocity of last iteration (input velocity) and gradient\n","    # 2. Update the weights using the new_velocity\n","    v = dW + beta*velocity\n","    new_W = W - learning_rate * v\n","    new_velocity = v\n","    # ==== end of code ====\n","    return new_W, new_velocity\n","\n","#######################################################################\n","# TODO: Set up model hyperparameters for SGD_Momentum\n","# Your hyperparameters should be identical to what you used for SGD (without momentum)#\n","#######################################################################\n","\n","# initialize model\n","model_SGD_Momentum = SoftmaxClassifier(hidden_dim = 300, weight_scale=1e-2)\n","\n","# start training\n","#Using SGD_Momentum as optimizer for trainning for training\n","model_SGD_Momentum, train_acc_history_SGD_Momentum, val_acc_history_SGD_Momentum = trainNetwork(\n","    model_SGD_Momentum, train_data,\n","    learning_rate = 0.05,\n","    lr_decay=0.9,\n","    batch_size=128,\n","    num_epochs=10,\n","     print_every=1000, optimizer = 'SGD_Momentum')\n","#######################################################################\n","#                         END OF YOUR CODE                            #\n","#######################################################################"]},{"cell_type":"markdown","metadata":{"id":"fcovGmpXvXXa"},"source":["# 4.2(e) Report Accuracy\n","\n","Run the given code and report the accuracy of model_SGD and model_SGD_Momentum on test set.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FwCq8pBhu6dz","executionInfo":{"status":"ok","timestamp":1696294919613,"user_tz":240,"elapsed":1767,"user":{"displayName":"Yiming Dou","userId":"00198832258301430987"}},"outputId":"d916e082-dd23-4909-ec6c-c4a1349ec278"},"outputs":[{"output_type":"stream","name":"stdout","text":["Test accuracy of model_SGD: 0.5166\n","Test accuracy of model_SGD_Momentum: 0.5052\n"]}],"source":["# report test accuracy\n","acc = testNetwork(model_SGD, data['X_test'], data['y_test'])\n","print(\"Test accuracy of model_SGD: {}\".format(acc))\n","# report test accuracy\n","acc = testNetwork(model_SGD_Momentum, data['X_test'], data['y_test'])\n","print(\"Test accuracy of model_SGD_Momentum: {}\".format(acc))"]},{"cell_type":"markdown","metadata":{"id":"oTrmbULS7i2N"},"source":["# 4.2(f) Plot\n","\n","Using the train_acc_history and val_acc_history, plot the train & val accuracy versus epochs on one plot, using SGD and SGD_Momentum as optimizer."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":454},"id":"SPjtnbya9S7g","executionInfo":{"status":"ok","timestamp":1696295270440,"user_tz":240,"elapsed":1069,"user":{"displayName":"Yiming Dou","userId":"00198832258301430987"}},"outputId":"45f5c348-883f-4c2f-d2b8-72b3a8d8c1c3"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}],"source":["#######################################################################\n","# Your Code here\n","#######################################################################\n","episode_x = np.array(range(len(train_acc_history_SGD)))\n","plt.plot(episode_x, train_acc_history_SGD, 'o-', color='r', label='Train_SGD')\n","plt.plot(episode_x, val_acc_history_SGD, 'o-', color='b', label='Val_SGD')\n","plt.plot(episode_x, train_acc_history_SGD_Momentum, 'o--', color='r', label='Train_SGD_Momentum')\n","plt.plot(episode_x, val_acc_history_SGD_Momentum, 'o--', color='b', label='Val_SGD_Momentum')\n","plt.legend(loc=\"lower right\")\n","plt.xlabel('Epochs')\n","plt.ylabel('Accuracy')\n","plt.show()\n","\n","#######################################################################\n","#                         END OF YOUR CODE                            #\n","#######################################################################"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":453},"id":"7D1JVByA-0t2","executionInfo":{"status":"error","timestamp":1696009103387,"user_tz":240,"elapsed":16047,"user":{"displayName":"Vishal Chandra","userId":"10884998298554150944"}},"outputId":"24a35b9f-3d4b-4519-dd8d-2eb00c6b0b35"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'bc5f1add34fef7c7f9fb83d3783311e2'...\n","remote: Enumerating objects: 10, done.\u001b[K\n","remote: Total 10 (delta 0), reused 0 (delta 0), pack-reused 10\u001b[K\n","Receiving objects: 100% (10/10), done.\n","Resolving deltas: 100% (3/3), done.\n"]},{"output_type":"error","ename":"MessageError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-17-515909b9b3b7>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# change the name to your ipynb file name shown on the top left of Colab window\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Important: make sure that your file name does not contain spaces!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mcolab_pdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'PS4solution442.ipynb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/content/colab_pdf.py\u001b[0m in \u001b[0;36mcolab_pdf\u001b[0;34m(file_name, notebookpath)\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgdrive_home\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrive_mount_point\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnotebookpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    130\u001b[0m   )\n\u001b[1;32m    131\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"]}],"source":["# generate pdf\n","# %%capture\n","!git clone https://gist.github.com/bc5f1add34fef7c7f9fb83d3783311e2.git\n","!cp bc5f1add34fef7c7f9fb83d3783311e2/colab_pdf.py colab_pdf.py\n","from colab_pdf import colab_pdf\n","# change the name to your ipynb file name shown on the top left of Colab window\n","# Important: make sure that your file name does not contain spaces!\n","colab_pdf('PS4solution442.ipynb')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lp_jAenDR_vi"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5Mf8Fw-1ZuRN"},"outputs":[],"source":["def f_1_torch(x0, x1, x2, w0, w1, w2, w3):\n","    \"\"\"\n","    Computes the forward and backward pass through the computational graph\n","    of (a)\n","\n","    Inputs:\n","    - x0, x1, w0, w1, w2: Python floats\n","\n","    Returns a tuple of:\n","    - L: The output of the graph\n","    - grads: A tuple (grad_x0, grad_x1, grad_w0, grad_w1, grad_w2)\n","    giving the derivative of the output L with respect to each input.\n","    \"\"\"\n","    ###########################################################################\n","    # TODO: Implement the forward pass for the computational graph for (a) and#\n","    # store the output of this graph as L                                     #\n","    ###########################################################################\n","    x0 = torch.tensor(x0, requires_grad=True)\n","    x1 = torch.tensor(x1, requires_grad=True)\n","    x2 = torch.tensor(x2, requires_grad=True)\n","    w0 = torch.tensor(w0, requires_grad=True)\n","    w1 = torch.tensor(w1, requires_grad=True)\n","    w2 = torch.tensor(w2, requires_grad=True)\n","    w3 = torch.tensor(w3, requires_grad=True)\n","\n","    a = x0 * w0\n","    b = w1 * x1\n","    c = 1.0 / x2\n","    d = w2 * c\n","    e = a + b\n","    f = e - d\n","    g = w3 + f\n","    h = g * (-1)\n","    i = torch.exp(h)\n","    j = i + 1\n","    L = 1.0 /j\n","    ###########################################################################\n","    #                              END OF YOUR CODE                           #\n","    ###########################################################################\n","\n","    ###########################################################################\n","    # TODO: Implement the backward pass for the computational graph for (a)   #\n","    # Store the gradients for each input                                      #\n","    ###########################################################################\n","    external_grad = torch.tensor(1.)\n","    L.backward(gradient=external_grad)\n","    grad_x0 = x0.grad\n","    grad_x1 = x1.grad\n","    grad_x2 = x2.grad\n","    grad_w0 = w0.grad\n","    grad_w1 = w1.grad\n","    grad_w2 = w2.grad\n","    grad_w3 = w3.grad\n","    ###########################################################################\n","    #                              END OF YOUR CODE                           #\n","    ###########################################################################\n","\n","    grads = (grad_x0, grad_x1, grad_x2, grad_w0, grad_w1, grad_w2, grad_w3)\n","    return L, grads"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}